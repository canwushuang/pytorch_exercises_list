{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7.PyTorch之小試牛刀.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNwRYxjCCugIbrMDTIdTur"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JuGm59xDKdF5"},"source":["1 PyTorch的核心是两个主要特征：\n","\n","一个n维张量，类似于numpy，但可以在GPU上运行\n","搭建和训练神经网络时的自动微分/求导机制"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eea_1CRmKf2_","executionInfo":{"status":"ok","timestamp":1632728047596,"user_tz":-480,"elapsed":3065,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}},"outputId":"ce923f18-3842-43af-b6a0-2dc58b0fcdd2"},"source":["#2.1 热身: Numpy\n","# -*- coding: utf-8 -*-\n","import numpy as np\n","\n","# N是批量大小; D_in是输入维度;\n","# 49/5000 H是隐藏的维度; D_out是输出维度。\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 创建随机输入和输出数据\n","x = np.random.randn(N, D_in)\n","y = np.random.randn(N, D_out)\n","\n","# 随机初始化权重\n","w1 = np.random.randn(D_in, H)\n","w2 = np.random.randn(H, D_out)\n","\n","learning_rate = 1e-6\n","for t in range(500):\n","    # 前向传递：计算预测值y\n","    h = x.dot(w1)\n","    h_relu = np.maximum(h, 0)\n","    y_pred = h_relu.dot(w2)\n","\n","    # 计算和打印损失loss\n","    loss = np.square(y_pred - y).sum()\n","    print(t, loss)\n","\n","    # 反向传播，计算w1和w2对loss的梯度\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_w2 = h_relu.T.dot(grad_y_pred)\n","    grad_h_relu = grad_y_pred.dot(w2.T)\n","    grad_h = grad_h_relu.copy()\n","    grad_h[h < 0] = 0\n","    grad_w1 = x.T.dot(grad_h)\n","\n","    # 更新权重\n","    w1 -= learning_rate * grad_w1\n","    w2 -= learning_rate * grad_w2"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["0 32610347.83910256\n","1 31481030.158171635\n","2 36562809.03275826\n","3 41567863.76937584\n","4 39576394.95865479\n","5 28610539.067062996\n","6 15485181.826075913\n","7 7009968.404996734\n","8 3205856.5662688995\n","9 1732319.1010808374\n","10 1140924.2145630023\n","11 860818.6177316656\n","12 696881.128518525\n","13 583149.9987396793\n","14 496152.2886697516\n","15 426376.14945097675\n","16 368919.9808512837\n","17 320957.86599938234\n","18 280550.7661567133\n","19 246359.13127831585\n","20 217232.79128029858\n","21 192301.020345519\n","22 170819.06011241034\n","23 152210.58118610116\n","24 136021.9636863892\n","25 121881.33740024027\n","26 109488.78007619324\n","27 98596.1005086803\n","28 88988.43864327311\n","29 80481.64012588188\n","30 72933.37717511618\n","31 66216.37342475733\n","32 60224.214430416185\n","33 54860.58934585938\n","34 50059.72403936277\n","35 45744.87830912559\n","36 41859.215882423916\n","37 38349.06772172271\n","38 35173.11603394158\n","39 32296.214203207877\n","40 29684.904125438607\n","41 27310.041533256895\n","42 25148.031994707402\n","43 23183.982877956707\n","44 21393.14235065977\n","45 19756.513889289818\n","46 18259.21446546872\n","47 16887.46010184796\n","48 15629.60555216157\n","49 14473.367700319372\n","50 13410.828057695739\n","51 12434.138474035744\n","52 11534.96199098545\n","53 10706.735627052942\n","54 9943.154153878571\n","55 9238.658594726761\n","56 8588.203500169475\n","57 7987.335988144285\n","58 7431.762335316458\n","59 6917.878114098118\n","60 6442.374672683995\n","61 6001.751822544962\n","62 5593.502790148191\n","63 5215.10091262307\n","64 4864.106308116976\n","65 4538.145823683533\n","66 4235.449454010628\n","67 3954.3073917722168\n","68 3692.9296345015237\n","69 3449.9073900505055\n","70 3223.86554652847\n","71 3013.52825914016\n","72 2817.784370978922\n","73 2635.832147626053\n","74 2466.2766015765155\n","75 2308.2101352625627\n","76 2160.8277672034733\n","77 2023.3941194558772\n","78 1895.1897636283113\n","79 1775.5047249697684\n","80 1663.7393932790142\n","81 1559.3233152439334\n","82 1461.7843294931617\n","83 1370.6192690194835\n","84 1285.4311035498654\n","85 1205.7424914987646\n","86 1131.2484857359877\n","87 1061.5325477237377\n","88 996.2887443012235\n","89 935.2311852128853\n","90 878.087030287833\n","91 824.5716640903472\n","92 774.4826438982591\n","93 727.5885500448331\n","94 683.6490022711173\n","95 642.4475096500619\n","96 603.8358719335929\n","97 567.621102845774\n","98 533.6577114668784\n","99 501.8101185541217\n","100 471.93076543532334\n","101 443.8876356070729\n","102 417.5733332056748\n","103 392.87782795035395\n","104 369.6894986554128\n","105 347.91581340936403\n","106 327.47002176569737\n","107 308.26478148055065\n","108 290.22085226107157\n","109 273.27119382271974\n","110 257.33751626990704\n","111 242.36432513091543\n","112 228.28617209733693\n","113 215.05134137098105\n","114 202.60477239643876\n","115 190.89629641839448\n","116 179.88797798745247\n","117 169.53184569981516\n","118 159.78586244811865\n","119 150.61504030476982\n","120 141.987257007619\n","121 133.86308319030567\n","122 126.21845266526871\n","123 119.0202046670842\n","124 112.24243413247842\n","125 105.86085540231903\n","126 99.85196811317616\n","127 94.1930227905711\n","128 88.86290336046463\n","129 83.84157351627378\n","130 79.1088905197131\n","131 74.65077410045257\n","132 70.44856902291542\n","133 66.48848027749588\n","134 62.75560588801995\n","135 59.23836907090627\n","136 55.9235820465159\n","137 52.79922126605611\n","138 49.85180821618512\n","139 47.0721477371018\n","140 44.45076163113604\n","141 41.97902678370643\n","142 39.64706518655344\n","143 37.44783063933275\n","144 35.37418274826366\n","145 33.41635872129983\n","146 31.56993086563928\n","147 29.826380065954883\n","148 28.181510474872823\n","149 26.629089230729555\n","150 25.163654324566366\n","151 23.78055645395628\n","152 22.47513526129515\n","153 21.242485032991482\n","154 20.078759567484763\n","155 18.97962061238083\n","156 17.941971666743456\n","157 16.96207330794164\n","158 16.036784757921147\n","159 15.16289403524728\n","160 14.337327744480696\n","161 13.557567748162054\n","162 12.820868532831504\n","163 12.12493926727787\n","164 11.467703558264258\n","165 10.84642137838869\n","166 10.25935160723789\n","167 9.704513128771165\n","168 9.180266294594297\n","169 8.684773968085345\n","170 8.216764627880103\n","171 7.774114726976041\n","172 7.355666297131357\n","173 6.960067856215481\n","174 6.586515535618306\n","175 6.233291149591979\n","176 5.899368472044317\n","177 5.583531896926775\n","178 5.284933715870613\n","179 5.002589371791188\n","180 4.735525579306463\n","181 4.482973279122302\n","182 4.244122640046033\n","183 4.01808022824308\n","184 3.804348042998194\n","185 3.6021316250338273\n","186 3.4108488614704298\n","187 3.2298606297555033\n","188 3.058580664342061\n","189 2.896499526216128\n","190 2.74319709560134\n","191 2.5981125496657764\n","192 2.4607874343997755\n","193 2.330834007929816\n","194 2.2078180628491526\n","195 2.0914146737619013\n","196 1.9812490869833517\n","197 1.8769729979130507\n","198 1.7782262931241595\n","199 1.6847386655780254\n","200 1.5962628982464402\n","201 1.5124924481225994\n","202 1.4332138946424355\n","203 1.3580901642234977\n","204 1.2869737774289485\n","205 1.2196351466153579\n","206 1.1558739026669222\n","207 1.095482888029304\n","208 1.0383087327416878\n","209 0.9841360321938767\n","210 0.9328243873199749\n","211 0.8842330586611605\n","212 0.8381952511339352\n","213 0.7945996462616522\n","214 0.7532902436832014\n","215 0.7141661031130386\n","216 0.6770888104944139\n","217 0.6419628488356457\n","218 0.6086858709668153\n","219 0.5771635305748131\n","220 0.5472922820973586\n","221 0.518982558531801\n","222 0.4921489213410444\n","223 0.4667208662667556\n","224 0.4426250277352337\n","225 0.41979148170416253\n","226 0.3981435974049656\n","227 0.37763603947780255\n","228 0.35819480449925956\n","229 0.33976569901624487\n","230 0.3222985897807972\n","231 0.30573312911046696\n","232 0.290033267231868\n","233 0.2751516392703651\n","234 0.26104204296488176\n","235 0.24766161752561977\n","236 0.23497892360778322\n","237 0.22295187818446052\n","238 0.21154905713297512\n","239 0.20073482269032927\n","240 0.19048252809677213\n","241 0.1807555311839413\n","242 0.17153206451796038\n","243 0.1627840651465785\n","244 0.1544898935236876\n","245 0.1466221283238542\n","246 0.1391622907087949\n","247 0.13208409309474622\n","248 0.12536924101228114\n","249 0.11899905398502536\n","250 0.11295707388995976\n","251 0.10722670535241362\n","252 0.10178986022037345\n","253 0.09663124301551726\n","254 0.09173595233981746\n","255 0.08709407299963555\n","256 0.08268782331168574\n","257 0.07850910046809109\n","258 0.07454269358551957\n","259 0.07077891272330693\n","260 0.06720733111763712\n","261 0.06381710156307382\n","262 0.060600695007479416\n","263 0.0575474272115238\n","264 0.054649368697833295\n","265 0.051900634088948586\n","266 0.04929074813230706\n","267 0.0468128113679022\n","268 0.044461185700809806\n","269 0.04222894448656073\n","270 0.040110183733419874\n","271 0.03809896883826902\n","272 0.036189955884928196\n","273 0.03437706228665778\n","274 0.032656316916765904\n","275 0.031022260405192476\n","276 0.02947193094648707\n","277 0.02799873477399325\n","278 0.02660016003476394\n","279 0.02527250176369677\n","280 0.02401141987939591\n","281 0.022814277390030356\n","282 0.021677156448004418\n","283 0.020597312480268984\n","284 0.019571884778448866\n","285 0.018598212434395595\n","286 0.01767329693270861\n","287 0.01679489983844693\n","288 0.015960469563805584\n","289 0.015168133788793583\n","290 0.014415485448957922\n","291 0.013700472538020668\n","292 0.013021562284567216\n","293 0.012376409110270716\n","294 0.011763485585231437\n","295 0.011181305927041595\n","296 0.010628274891470477\n","297 0.010102755859909997\n","298 0.009603615789707885\n","299 0.00912927763590445\n","300 0.0086785958159872\n","301 0.00825042420583489\n","302 0.007843680111733012\n","303 0.00745705900125945\n","304 0.007089732834507053\n","305 0.006740735331793994\n","306 0.006408994675061224\n","307 0.006093691219829161\n","308 0.005794116771755018\n","309 0.005509384696737319\n","310 0.00523883804333373\n","311 0.004981694135316468\n","312 0.004737249903579972\n","313 0.0045048983505436525\n","314 0.0042841433025769295\n","315 0.004074321798539521\n","316 0.003874801884241895\n","317 0.003685189618812248\n","318 0.0035049126870112467\n","319 0.0033335124962758545\n","320 0.0031705957414415115\n","321 0.0030156954472538057\n","322 0.002868485345743703\n","323 0.002728507694020094\n","324 0.002595408148908587\n","325 0.0024688724409795636\n","326 0.002348586518134436\n","327 0.002234187327846476\n","328 0.0021254151933659433\n","329 0.0020219752466824454\n","330 0.0019236348046738246\n","331 0.0018300904489345492\n","332 0.0017411711290693376\n","333 0.0016565876539284883\n","334 0.0015761703845259256\n","335 0.0014996998729394284\n","336 0.0014269759338748386\n","337 0.0013578015603611718\n","338 0.001292001144708554\n","339 0.0012294120507078772\n","340 0.0011698788805502261\n","341 0.0011132615731273472\n","342 0.0010594259835279727\n","343 0.0010081965801092558\n","344 0.000959459092035247\n","345 0.0009131098236568784\n","346 0.0008690089343951655\n","347 0.0008270771612117751\n","348 0.0007871731652964772\n","349 0.0007492144702954037\n","350 0.0007130997541767339\n","351 0.0006787481738509334\n","352 0.00064606039363423\n","353 0.0006149550545101578\n","354 0.0005853629109968139\n","355 0.0005572172042828236\n","356 0.0005304251917066439\n","357 0.0005049352184054806\n","358 0.0004806879038193164\n","359 0.0004576018961164218\n","360 0.000435640371030761\n","361 0.0004147405242312197\n","362 0.0003948492434300818\n","363 0.00037591707547420265\n","364 0.00035790296567435125\n","365 0.00034076582675038797\n","366 0.00032444851632634205\n","367 0.0003089258918336961\n","368 0.00029414629136454123\n","369 0.0002800794656210019\n","370 0.0002666895118938747\n","371 0.00025394763444230016\n","372 0.00024181969847540457\n","373 0.0002302754921789623\n","374 0.00021928584023525168\n","375 0.00020882672814014996\n","376 0.00019887143382671214\n","377 0.00018939121642357865\n","378 0.0001803679440492736\n","379 0.00017177809890655984\n","380 0.00016360029465216987\n","381 0.00015581410069182879\n","382 0.0001484009058994871\n","383 0.0001413440492059087\n","384 0.0001346235751189614\n","385 0.0001282270829808271\n","386 0.00012213691173988098\n","387 0.00011633771855656135\n","388 0.00011081551962051945\n","389 0.00010555692644405651\n","390 0.00010055058061948\n","391 9.578312908085138e-05\n","392 9.124393654286955e-05\n","393 8.692107991827078e-05\n","394 8.280473144364192e-05\n","395 7.888687851173056e-05\n","396 7.515383192685095e-05\n","397 7.159844175279915e-05\n","398 6.821174619159857e-05\n","399 6.498730141087185e-05\n","400 6.191607208649368e-05\n","401 5.8991089147383375e-05\n","402 5.620562801399582e-05\n","403 5.355256711644073e-05\n","404 5.102590478091401e-05\n","405 4.861864753786497e-05\n","406 4.63257728500363e-05\n","407 4.4142079918102455e-05\n","408 4.206207792854426e-05\n","409 4.008079447934815e-05\n","410 3.819320316209875e-05\n","411 3.639499596100278e-05\n","412 3.4682330002500536e-05\n","413 3.30507887620233e-05\n","414 3.149659524152165e-05\n","415 3.0016455262531086e-05\n","416 2.860587178449091e-05\n","417 2.726199924271203e-05\n","418 2.5981490520091577e-05\n","419 2.476177680344426e-05\n","420 2.359950841512472e-05\n","421 2.2492630725853687e-05\n","422 2.143756051779092e-05\n","423 2.0432220112245105e-05\n","424 1.947453161626029e-05\n","425 1.856188396395065e-05\n","426 1.7692376055679346e-05\n","427 1.686405563511573e-05\n","428 1.6074557173138284e-05\n","429 1.5322404393544123e-05\n","430 1.4605467419204606e-05\n","431 1.392231196564116e-05\n","432 1.3271406252960477e-05\n","433 1.2650988164268217e-05\n","434 1.2059878304251837e-05\n","435 1.1496556653758044e-05\n","436 1.0959805045118626e-05\n","437 1.0448335815504885e-05\n","438 9.960761488866273e-06\n","439 9.49601697584892e-06\n","440 9.0531152708924e-06\n","441 8.630905626672518e-06\n","442 8.228604155248917e-06\n","443 7.845127522017147e-06\n","444 7.479598624113683e-06\n","445 7.13132145403962e-06\n","446 6.7992940948022675e-06\n","447 6.482829534558866e-06\n","448 6.181145112743372e-06\n","449 5.8936040021972694e-06\n","450 5.619523948604065e-06\n","451 5.358204016051981e-06\n","452 5.109154675656798e-06\n","453 4.871806850267928e-06\n","454 4.645464238473542e-06\n","455 4.429735935483908e-06\n","456 4.224090003682288e-06\n","457 4.028010178031038e-06\n","458 3.841120015680647e-06\n","459 3.6629247610614074e-06\n","460 3.493064028915123e-06\n","461 3.3311194853563777e-06\n","462 3.1767027136277984e-06\n","463 3.0294917618597313e-06\n","464 2.8891428461717095e-06\n","465 2.7553090114995905e-06\n","466 2.6277502023143395e-06\n","467 2.5060924325788484e-06\n","468 2.3901177917965596e-06\n","469 2.2795338568960913e-06\n","470 2.1741180149910904e-06\n","471 2.0735774188496223e-06\n","472 1.9776988317627187e-06\n","473 1.8862690426768394e-06\n","474 1.7991128725720254e-06\n","475 1.7160015560779638e-06\n","476 1.6367465326462376e-06\n","477 1.5612015080458642e-06\n","478 1.4891228906661695e-06\n","479 1.4203910166289042e-06\n","480 1.3548627366880024e-06\n","481 1.2923618431226433e-06\n","482 1.2327701299703044e-06\n","483 1.1759350421702126e-06\n","484 1.1217165869204025e-06\n","485 1.0700233990034788e-06\n","486 1.0207188424683865e-06\n","487 9.73697181098256e-07\n","488 9.288584770683227e-07\n","489 8.860850494878544e-07\n","490 8.453104839216454e-07\n","491 8.064154037869136e-07\n","492 7.693079654459141e-07\n","493 7.339194381249393e-07\n","494 7.001694856107733e-07\n","495 6.679704954960464e-07\n","496 6.372658273433802e-07\n","497 6.079807203771193e-07\n","498 5.800517895670835e-07\n","499 5.534061505053858e-07\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oseiuc7IKnzv","executionInfo":{"status":"ok","timestamp":1632728052966,"user_tz":-480,"elapsed":5373,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}},"outputId":"bfb8b2ba-dd00-4a9f-dc7c-2292f210e821"},"source":["#2.2 PyTorch：张量\n","# -*- coding: utf-8 -*-\n","\n","import torch\n","\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device（“cuda：0”）＃取消注释以在GPU上运行\n","\n","# N是批量大小; D_in是输入维度;\n","# H是隐藏的维度; D_out是输出维度。\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","#创建随机输入和输出数据\n","x = torch.randn(N, D_in, device=device, dtype=dtype)\n","y = torch.randn(N, D_out, device=device, dtype=dtype)\n","\n","# 随机初始化权重\n","w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n","w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n","\n","learning_rate = 1e-6\n","for t in range(500):\n","    # 前向传递：计算预测y\n","    h = x.mm(w1)\n","    h_relu = h.clamp(min=0)\n","    y_pred = h_relu.mm(w2)\n","\n","    # 计算和打印损失\n","    loss = (y_pred - y).pow(2).sum().item()\n","    print(t, loss)\n","\n","    # Backprop计算w1和w2相对于损耗的梯度\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_w2 = h_relu.t().mm(grad_y_pred)\n","    grad_h_relu = grad_y_pred.mm(w2.t())\n","    grad_h = grad_h_relu.clone()\n","    grad_h[h < 0] = 0\n","    grad_w1 = x.t().mm(grad_h)\n","\n","    # 使用梯度下降更新权重\n","    w1 -= learning_rate * grad_w1\n","    w2 -= learning_rate * grad_w2"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["0 39108028.0\n","1 37143820.0\n","2 36847392.0\n","3 32401282.0\n","4 23232070.0\n","5 13681248.0\n","6 7185313.5\n","7 3807221.0\n","8 2230244.75\n","9 1485991.375\n","10 1098818.75\n","11 868839.875\n","12 713908.0\n","13 599472.5\n","14 510066.25\n","15 437870.625\n","16 378480.5\n","17 328962.15625\n","18 287289.375\n","19 251964.78125\n","20 221842.90625\n","21 196033.25\n","22 173785.703125\n","23 154539.0\n","24 137845.078125\n","25 123288.734375\n","26 110550.828125\n","27 99364.8515625\n","28 89512.5703125\n","29 80814.0546875\n","30 73116.6796875\n","31 66280.5546875\n","32 60193.23828125\n","33 54757.8203125\n","34 49908.2578125\n","35 45561.8046875\n","36 41658.02734375\n","37 38144.9921875\n","38 34983.16796875\n","39 32127.0234375\n","40 29547.90234375\n","41 27209.55078125\n","42 25086.708984375\n","43 23156.73828125\n","44 21399.21484375\n","45 19796.595703125\n","46 18332.515625\n","47 16994.51953125\n","48 15769.4326171875\n","49 14646.900390625\n","50 13616.8193359375\n","51 12668.365234375\n","52 11796.4892578125\n","53 10993.671875\n","54 10253.462890625\n","55 9570.8173828125\n","56 8940.279296875\n","57 8357.2958984375\n","58 7817.7236328125\n","59 7318.01904296875\n","60 6855.3681640625\n","61 6426.05712890625\n","62 6027.33154296875\n","63 5656.888671875\n","64 5312.2646484375\n","65 4991.45458984375\n","66 4692.47607421875\n","67 4413.82421875\n","68 4153.7685546875\n","69 3910.891845703125\n","70 3684.073974609375\n","71 3471.89013671875\n","72 3273.37109375\n","73 3087.506103515625\n","74 2913.37890625\n","75 2750.2490234375\n","76 2597.287841796875\n","77 2453.757080078125\n","78 2318.95263671875\n","79 2192.350830078125\n","80 2073.373046875\n","81 1961.45068359375\n","82 1856.166259765625\n","83 1757.0970458984375\n","84 1663.77734375\n","85 1575.8699951171875\n","86 1493.0074462890625\n","87 1414.921630859375\n","88 1341.2821044921875\n","89 1271.802978515625\n","90 1206.1973876953125\n","91 1144.238037109375\n","92 1085.7286376953125\n","93 1030.40673828125\n","94 978.11572265625\n","95 928.6889038085938\n","96 881.943115234375\n","97 837.6942138671875\n","98 795.8130493164062\n","99 756.168212890625\n","100 718.6217651367188\n","101 683.0859985351562\n","102 649.4580688476562\n","103 617.58203125\n","104 587.3485717773438\n","105 558.6919555664062\n","106 531.5052490234375\n","107 505.72235107421875\n","108 481.2474670410156\n","109 458.0227355957031\n","110 435.9755554199219\n","111 415.04290771484375\n","112 395.168212890625\n","113 376.28271484375\n","114 358.33929443359375\n","115 341.2923278808594\n","116 325.0902404785156\n","117 309.6964111328125\n","118 295.05438232421875\n","119 281.13543701171875\n","120 267.8970031738281\n","121 255.30752563476562\n","122 243.33062744140625\n","123 231.93856811523438\n","124 221.09950256347656\n","125 210.78158569335938\n","126 200.9624481201172\n","127 191.61940002441406\n","128 182.72100830078125\n","129 174.24940490722656\n","130 166.18167114257812\n","131 158.50193786621094\n","132 151.19090270996094\n","133 144.22000122070312\n","134 137.58163452148438\n","135 131.25918579101562\n","136 125.23542785644531\n","137 119.49263000488281\n","138 114.02056884765625\n","139 108.8099365234375\n","140 103.84083557128906\n","141 99.103515625\n","142 94.58802032470703\n","143 90.28330993652344\n","144 86.17899322509766\n","145 82.26514434814453\n","146 78.53447723388672\n","147 74.97613525390625\n","148 71.58183288574219\n","149 68.34539031982422\n","150 65.25931549072266\n","151 62.31282043457031\n","152 59.50446319580078\n","153 56.823814392089844\n","154 54.26617431640625\n","155 51.82775115966797\n","156 49.50037384033203\n","157 47.27863311767578\n","158 45.15885925292969\n","159 43.136287689208984\n","160 41.20519256591797\n","161 39.36300277709961\n","162 37.604217529296875\n","163 35.92527770996094\n","164 34.32291030883789\n","165 32.79234313964844\n","166 31.332242965698242\n","167 29.93784523010254\n","168 28.606483459472656\n","169 27.33548927307129\n","170 26.121410369873047\n","171 24.96258544921875\n","172 23.856002807617188\n","173 22.798656463623047\n","174 21.788875579833984\n","175 20.825448989868164\n","176 19.90452003479004\n","177 19.025110244750977\n","178 18.18549346923828\n","179 17.38332748413086\n","180 16.616474151611328\n","181 15.884236335754395\n","182 15.185052871704102\n","183 14.517436981201172\n","184 13.878998756408691\n","185 13.269227981567383\n","186 12.686858177185059\n","187 12.130115509033203\n","188 11.598404884338379\n","189 11.089984893798828\n","190 10.604063034057617\n","191 10.140043258666992\n","192 9.69659423828125\n","193 9.272558212280273\n","194 8.867748260498047\n","195 8.480642318725586\n","196 8.11031723022461\n","197 7.756599426269531\n","198 7.418485641479492\n","199 7.095174789428711\n","200 6.786398887634277\n","201 6.490696907043457\n","202 6.208954334259033\n","203 5.938767910003662\n","204 5.681000232696533\n","205 5.434345722198486\n","206 5.198266983032227\n","207 4.9730329513549805\n","208 4.757314205169678\n","209 4.551246643066406\n","210 4.354327201843262\n","211 4.1659626960754395\n","212 3.985579490661621\n","213 3.813524007797241\n","214 3.648709774017334\n","215 3.4912471771240234\n","216 3.3405065536499023\n","217 3.196354389190674\n","218 3.0585525035858154\n","219 2.9267163276672363\n","220 2.8006465435028076\n","221 2.6801230907440186\n","222 2.5647201538085938\n","223 2.454594135284424\n","224 2.3489363193511963\n","225 2.2479748725891113\n","226 2.15151309967041\n","227 2.0591416358947754\n","228 1.970754623413086\n","229 1.8863105773925781\n","230 1.80544114112854\n","231 1.7281057834625244\n","232 1.654008388519287\n","233 1.5833215713500977\n","234 1.5156103372573853\n","235 1.4507626295089722\n","236 1.3888435363769531\n","237 1.3294367790222168\n","238 1.2727774381637573\n","239 1.218574047088623\n","240 1.166472315788269\n","241 1.1167426109313965\n","242 1.0691989660263062\n","243 1.0235979557037354\n","244 0.9801353216171265\n","245 0.9383419752120972\n","246 0.8984273076057434\n","247 0.860287070274353\n","248 0.8236457109451294\n","249 0.7886680960655212\n","250 0.7552096843719482\n","251 0.7231318950653076\n","252 0.6924050450325012\n","253 0.6631463170051575\n","254 0.6350558996200562\n","255 0.6081477999687195\n","256 0.5823870897293091\n","257 0.5577064752578735\n","258 0.5342059135437012\n","259 0.5115939378738403\n","260 0.48989230394363403\n","261 0.4692375659942627\n","262 0.44938191771507263\n","263 0.4303547441959381\n","264 0.4122947156429291\n","265 0.3948986530303955\n","266 0.37827637791633606\n","267 0.36234235763549805\n","268 0.34708553552627563\n","269 0.3324885964393616\n","270 0.3184177577495575\n","271 0.30505236983299255\n","272 0.29222050309181213\n","273 0.27992919087409973\n","274 0.26814374327659607\n","275 0.25688615441322327\n","276 0.24610725045204163\n","277 0.23581305146217346\n","278 0.22592881321907043\n","279 0.21643592417240143\n","280 0.20737600326538086\n","281 0.1986902505159378\n","282 0.19036686420440674\n","283 0.1823808252811432\n","284 0.1747625470161438\n","285 0.1674375683069229\n","286 0.16044649481773376\n","287 0.1537880152463913\n","288 0.14734342694282532\n","289 0.1412036120891571\n","290 0.135298952460289\n","291 0.1296357363462448\n","292 0.12424624711275101\n","293 0.11902594566345215\n","294 0.11405925452709198\n","295 0.10932175815105438\n","296 0.10477687418460846\n","297 0.10041503608226776\n","298 0.09626547992229462\n","299 0.09226694703102112\n","300 0.08842771500349045\n","301 0.08476097881793976\n","302 0.0812193751335144\n","303 0.07787328213453293\n","304 0.07463512569665909\n","305 0.0715564712882042\n","306 0.06858603656291962\n","307 0.06576178967952728\n","308 0.06304221600294113\n","309 0.06044669821858406\n","310 0.057928550988435745\n","311 0.05553119629621506\n","312 0.053242094814777374\n","313 0.05103101581335068\n","314 0.04889998957514763\n","315 0.04690539091825485\n","316 0.04496945068240166\n","317 0.043123457580804825\n","318 0.04133931174874306\n","319 0.03965882584452629\n","320 0.038021232932806015\n","321 0.03646964952349663\n","322 0.034962572157382965\n","323 0.03354206308722496\n","324 0.032178543508052826\n","325 0.030847933143377304\n","326 0.02957790531218052\n","327 0.028368089348077774\n","328 0.02720431610941887\n","329 0.026099717244505882\n","330 0.025031181052327156\n","331 0.02401287481188774\n","332 0.02303837426006794\n","333 0.022106412798166275\n","334 0.02119370736181736\n","335 0.020340953022241592\n","336 0.019514484331011772\n","337 0.018726613372564316\n","338 0.017957616597414017\n","339 0.017239730805158615\n","340 0.016554415225982666\n","341 0.01588132604956627\n","342 0.015243012458086014\n","343 0.014624289236962795\n","344 0.014042478986084461\n","345 0.01347406581044197\n","346 0.01293913647532463\n","347 0.012421348132193089\n","348 0.011923439800739288\n","349 0.011441252194344997\n","350 0.010979766957461834\n","351 0.010551601648330688\n","352 0.01012779027223587\n","353 0.009728807955980301\n","354 0.009342154487967491\n","355 0.008970916271209717\n","356 0.008619999513030052\n","357 0.008278590627014637\n","358 0.007953730411827564\n","359 0.007639232557266951\n","360 0.0073435078375041485\n","361 0.007064139470458031\n","362 0.006788301281630993\n","363 0.006525979842990637\n","364 0.0062709650956094265\n","365 0.006027684081345797\n","366 0.005798738915473223\n","367 0.005576420575380325\n","368 0.005356833338737488\n","369 0.00515526719391346\n","370 0.0049618869088590145\n","371 0.004771129228174686\n","372 0.004587513394653797\n","373 0.004411861300468445\n","374 0.004246158991008997\n","375 0.004084619227796793\n","376 0.003936070017516613\n","377 0.003791597904637456\n","378 0.003647527890279889\n","379 0.0035116015933454037\n","380 0.003383909584954381\n","381 0.00326147791929543\n","382 0.0031433049589395523\n","383 0.003032710636034608\n","384 0.0029222946614027023\n","385 0.0028137839399278164\n","386 0.002712914254516363\n","387 0.002613713964819908\n","388 0.00252339243888855\n","389 0.002432596404105425\n","390 0.0023480108939111233\n","391 0.002263953909277916\n","392 0.002186295110732317\n","393 0.002111137146130204\n","394 0.0020386562682688236\n","395 0.0019685400184243917\n","396 0.0019005105132237077\n","397 0.001833962625823915\n","398 0.0017706414218991995\n","399 0.0017148174811154604\n","400 0.0016567475395277143\n","401 0.0015984040219336748\n","402 0.0015455451793968678\n","403 0.0014953098725527525\n","404 0.0014455362688750029\n","405 0.0013995022745802999\n","406 0.0013533870223909616\n","407 0.0013114429311826825\n","408 0.0012692478485405445\n","409 0.0012266917619854212\n","410 0.0011881914688274264\n","411 0.001151773612946272\n","412 0.0011153507512062788\n","413 0.0010817383881658316\n","414 0.0010465476661920547\n","415 0.0010144570842385292\n","416 0.0009832497453317046\n","417 0.0009549343958497047\n","418 0.0009260692168027163\n","419 0.00089977344032377\n","420 0.0008729703258723021\n","421 0.0008453439804725349\n","422 0.0008211981039494276\n","423 0.0007967688725329936\n","424 0.0007742032175883651\n","425 0.0007529295980930328\n","426 0.0007306892657652497\n","427 0.000710513792000711\n","428 0.0006908419309183955\n","429 0.0006704701809212565\n","430 0.000652812363114208\n","431 0.0006340960972011089\n","432 0.0006171187269501388\n","433 0.0006007585907354951\n","434 0.0005852457834407687\n","435 0.0005686157965101302\n","436 0.0005537319811992347\n","437 0.0005391543381847441\n","438 0.0005247719818726182\n","439 0.0005105064483359456\n","440 0.0004979430814273655\n","441 0.0004844501963816583\n","442 0.00047243357403203845\n","443 0.00045981293078511953\n","444 0.0004479558556340635\n","445 0.00043612232548184693\n","446 0.0004254415398463607\n","447 0.0004147313884459436\n","448 0.0004049520648550242\n","449 0.0003940821043215692\n","450 0.00038431494613178074\n","451 0.00037642125971615314\n","452 0.00036732424632646143\n","453 0.0003589934785850346\n","454 0.0003499647427815944\n","455 0.00034139034687541425\n","456 0.0003335298097226769\n","457 0.00032613248913548887\n","458 0.0003186555113643408\n","459 0.00031179783400148153\n","460 0.00030458366381935775\n","461 0.000298461876809597\n","462 0.0002924510627053678\n","463 0.0002849061565939337\n","464 0.0002794752945192158\n","465 0.0002729704137891531\n","466 0.0002665704523678869\n","467 0.00026144442381337285\n","468 0.00025518599431961775\n","469 0.00024963912437669933\n","470 0.00024479394778609276\n","471 0.00023932120529934764\n","472 0.0002347153495065868\n","473 0.00023062407854013145\n","474 0.00022594982874579728\n","475 0.00022022549819666892\n","476 0.0002159895666409284\n","477 0.00021207809913903475\n","478 0.0002072142669931054\n","479 0.0002033657510764897\n","480 0.00019938140758313239\n","481 0.0001950446458067745\n","482 0.00019101009820587933\n","483 0.00018727005226537585\n","484 0.00018374690262135118\n","485 0.0001806452637538314\n","486 0.0001771733514033258\n","487 0.00017436251800972968\n","488 0.0001706504845060408\n","489 0.00016715022502467036\n","490 0.0001635079679545015\n","491 0.00016010763647500426\n","492 0.00015732445172034204\n","493 0.0001540175435366109\n","494 0.00015127792721614242\n","495 0.0001490091235609725\n","496 0.0001462096261093393\n","497 0.0001428701070835814\n","498 0.00014103634748607874\n","499 0.00013851727999281138\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqVBVtA0Kref","executionInfo":{"status":"ok","timestamp":1632728054586,"user_tz":-480,"elapsed":1621,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}},"outputId":"eed9e6a5-9acc-41df-963d-7beb00c7afc9"},"source":["#3.自动求导\n","#3.1 PyTorch：张量和自动求导\n","# -*- coding: utf-8 -*-\n","import torch\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device（“cuda：0”）＃取消注释以在GPU上运行\n","\n","# N是批量大小; D_in是输入维度;\n","# H是隐藏的维度; D_out是输出维度。\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 创建随机Tensors以保持输入和输出。\n","# 设置requires_grad = False表示我们不需要计算渐变\n","# 在向后传球期间对于这些Tensors。\n","x = torch.randn(N, D_in, device=device, dtype=dtype)\n","y = torch.randn(N, D_out, device=device, dtype=dtype)\n","\n","# 为权重创建随机Tensors。\n","# 设置requires_grad = True表示我们想要计算渐变\n","# 在向后传球期间尊重这些张贴。\n","w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n","w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n","\n","learning_rate = 1e-6\n","for t in range(500):\n","    # 前向传播：使用tensors上的操作计算预测值y; \n","      # 由于w1和w2有requires_grad=True，涉及这些张量的操作将让PyTorch构建计算图，\n","    # 从而允许自动计算梯度。由于我们不再手工实现反向传播，所以不需要保留中间值的引用。\n","    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n","\n","    # 使用Tensors上的操作计算和打印丢失。\n","    # loss是一个形状为()的张量\n","    # loss.item() 得到这个张量对应的python数值\n","    loss = (y_pred - y).pow(2).sum()\n","    print(t, loss.item())\n","\n","    # 使用autograd计算反向传播。这个调用将计算loss对所有requires_grad=True的tensor的梯度。\n","    # 这次调用后，w1.grad和w2.grad将分别是loss对w1和w2的梯度张量。\n","    loss.backward()\n","\n","    # 使用梯度下降更新权重。对于这一步，我们只想对w1和w2的值进行原地改变；不想为更新阶段构建计算图，\n","    # 所以我们使用torch.no_grad()上下文管理器防止PyTorch为更新构建计算图\n","    with torch.no_grad():\n","        w1 -= learning_rate * w1.grad\n","        w2 -= learning_rate * w2.grad\n","\n","        # 反向传播后手动将梯度设置为零\n","        w1.grad.zero_()\n","        w2.grad.zero_()"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0 31108264.0\n","1 25021230.0\n","2 24259818.0\n","3 24490436.0\n","4 23358372.0\n","5 19510584.0\n","6 14197279.0\n","7 9042975.0\n","8 5397660.5\n","9 3171307.75\n","10 1947353.5\n","11 1281722.625\n","12 912516.125\n","13 694322.125\n","14 555342.125\n","15 459427.125\n","16 388545.6875\n","17 333425.0625\n","18 288972.0\n","19 252181.859375\n","20 221259.828125\n","21 194944.5625\n","22 172379.53125\n","23 152908.5\n","24 136030.8125\n","25 121334.6484375\n","26 108493.2734375\n","27 97242.7265625\n","28 87343.65625\n","29 78608.1015625\n","30 70887.2578125\n","31 64047.078125\n","32 57971.24609375\n","33 52557.6015625\n","34 47726.3828125\n","35 43400.9921875\n","36 39522.4921875\n","37 36036.20703125\n","38 32896.05859375\n","39 30066.63671875\n","40 27509.146484375\n","41 25193.96484375\n","42 23096.091796875\n","43 21192.556640625\n","44 19462.560546875\n","45 17889.078125\n","46 16455.998046875\n","47 15149.703125\n","48 13957.271484375\n","49 12867.640625\n","50 11872.2568359375\n","51 10962.24609375\n","52 10131.185546875\n","53 9368.806640625\n","54 8669.0390625\n","55 8026.18701171875\n","56 7434.666015625\n","57 6890.333984375\n","58 6388.99951171875\n","59 5927.958984375\n","60 5503.0087890625\n","61 5110.75341796875\n","62 4748.5927734375\n","63 4414.07421875\n","64 4104.62109375\n","65 3818.400634765625\n","66 3553.559326171875\n","67 3308.353271484375\n","68 3081.1669921875\n","69 2870.4921875\n","70 2675.398193359375\n","71 2494.480224609375\n","72 2326.638916015625\n","73 2170.677978515625\n","74 2025.7637939453125\n","75 1891.138916015625\n","76 1765.9710693359375\n","77 1649.770751953125\n","78 1541.63818359375\n","79 1440.98046875\n","80 1347.2197265625\n","81 1259.872314453125\n","82 1178.523193359375\n","83 1102.6968994140625\n","84 1031.9534912109375\n","85 965.9710693359375\n","86 904.4229736328125\n","87 846.976806640625\n","88 793.3424682617188\n","89 743.2648315429688\n","90 696.5078125\n","91 652.8063354492188\n","92 611.986572265625\n","93 573.8125\n","94 538.1029052734375\n","95 504.7413024902344\n","96 473.5171203613281\n","97 444.3173522949219\n","98 416.98675537109375\n","99 391.4052429199219\n","100 367.444580078125\n","101 345.0232849121094\n","102 324.0115966796875\n","103 304.3367919921875\n","104 285.88751220703125\n","105 268.60723876953125\n","106 252.40548706054688\n","107 237.2152557373047\n","108 222.97366333007812\n","109 209.61184692382812\n","110 197.0819854736328\n","111 185.32537841796875\n","112 174.2952423095703\n","113 163.94793701171875\n","114 154.23765563964844\n","115 145.1145477294922\n","116 136.5467071533203\n","117 128.50376892089844\n","118 120.94715881347656\n","119 113.849365234375\n","120 107.17923736572266\n","121 100.91976928710938\n","122 95.028564453125\n","123 89.49198150634766\n","124 84.28941345214844\n","125 79.3982162475586\n","126 74.79930877685547\n","127 70.47132110595703\n","128 66.40119171142578\n","129 62.5748176574707\n","130 58.97438430786133\n","131 55.585838317871094\n","132 52.39606475830078\n","133 49.3942985534668\n","134 46.56904983520508\n","135 43.908836364746094\n","136 41.40510177612305\n","137 39.04839324951172\n","138 36.82994079589844\n","139 34.74032974243164\n","140 32.771278381347656\n","141 30.91679573059082\n","142 29.17157745361328\n","143 27.524749755859375\n","144 25.974212646484375\n","145 24.51291847229004\n","146 23.135499954223633\n","147 21.83707618713379\n","148 20.613189697265625\n","149 19.45942497253418\n","150 18.371912002563477\n","151 17.345853805541992\n","152 16.37866973876953\n","153 15.466586112976074\n","154 14.605520248413086\n","155 13.793968200683594\n","156 13.027898788452148\n","157 12.305156707763672\n","158 11.623320579528809\n","159 10.980870246887207\n","160 10.373494148254395\n","161 9.801322937011719\n","162 9.260931968688965\n","163 8.751065254211426\n","164 8.2693452835083\n","165 7.814675331115723\n","166 7.385210990905762\n","167 6.979706764221191\n","168 6.597124099731445\n","169 6.235714435577393\n","170 5.8945112228393555\n","171 5.572299480438232\n","172 5.268133163452148\n","173 4.980573654174805\n","174 4.708920001983643\n","175 4.452385902404785\n","176 4.210207939147949\n","177 3.9813578128814697\n","178 3.765186071395874\n","179 3.5607755184173584\n","180 3.3674981594085693\n","181 3.1851184368133545\n","182 3.012753963470459\n","183 2.849655866622925\n","184 2.6956331729888916\n","185 2.5498898029327393\n","186 2.4123849868774414\n","187 2.2822012901306152\n","188 2.159057140350342\n","189 2.0427446365356445\n","190 1.932806372642517\n","191 1.8288347721099854\n","192 1.7305397987365723\n","193 1.6374462842941284\n","194 1.5496141910552979\n","195 1.4664607048034668\n","196 1.3878908157348633\n","197 1.3136178255081177\n","198 1.2432594299316406\n","199 1.176755428314209\n","200 1.113774299621582\n","201 1.0542973279953003\n","202 0.9980686902999878\n","203 0.9448108673095703\n","204 0.8943626284599304\n","205 0.8466687202453613\n","206 0.8016254305839539\n","207 0.7588348388671875\n","208 0.7185304760932922\n","209 0.6803222894668579\n","210 0.6442040205001831\n","211 0.6099683046340942\n","212 0.5774890184402466\n","213 0.5469101071357727\n","214 0.5178847908973694\n","215 0.490464985370636\n","216 0.46449777483940125\n","217 0.4398404359817505\n","218 0.41656067967414856\n","219 0.39458414912223816\n","220 0.3737048804759979\n","221 0.3539750874042511\n","222 0.33522844314575195\n","223 0.3175204396247864\n","224 0.300771564245224\n","225 0.28494444489479065\n","226 0.2699161767959595\n","227 0.25568249821662903\n","228 0.2422298789024353\n","229 0.22941969335079193\n","230 0.21735192835330963\n","231 0.20590847730636597\n","232 0.19510620832443237\n","233 0.18484489619731903\n","234 0.1751110702753067\n","235 0.1659185290336609\n","236 0.15719980001449585\n","237 0.1489834040403366\n","238 0.14118215441703796\n","239 0.13375896215438843\n","240 0.1267533004283905\n","241 0.12013424932956696\n","242 0.11383470147848129\n","243 0.10785958915948868\n","244 0.10220950096845627\n","245 0.09686393290758133\n","246 0.09181547164916992\n","247 0.0870191901922226\n","248 0.08247137069702148\n","249 0.07815282791852951\n","250 0.07407447695732117\n","251 0.07021044939756393\n","252 0.06655744463205338\n","253 0.06306829303503036\n","254 0.059809450060129166\n","255 0.05668382719159126\n","256 0.05370892584323883\n","257 0.05092760547995567\n","258 0.04826515540480614\n","259 0.04575914889574051\n","260 0.043356217443943024\n","261 0.04110211879014969\n","262 0.038978561758995056\n","263 0.036952827125787735\n","264 0.03505219891667366\n","265 0.03324368968605995\n","266 0.031523238867521286\n","267 0.029894554987549782\n","268 0.02835269831120968\n","269 0.02687866799533367\n","270 0.02548488974571228\n","271 0.024170275777578354\n","272 0.02293029986321926\n","273 0.021748634055256844\n","274 0.02062078006565571\n","275 0.01956389658153057\n","276 0.0185568705201149\n","277 0.01759725995361805\n","278 0.01668412797152996\n","279 0.015832990407943726\n","280 0.01503524836152792\n","281 0.014258144423365593\n","282 0.013538336381316185\n","283 0.012849337421357632\n","284 0.012193560600280762\n","285 0.011567484587430954\n","286 0.010988734662532806\n","287 0.01042611338198185\n","288 0.009899702854454517\n","289 0.00940371211618185\n","290 0.008919750340282917\n","291 0.008478369563817978\n","292 0.008056312799453735\n","293 0.007647257298231125\n","294 0.0072732544504106045\n","295 0.006906117312610149\n","296 0.006559778470546007\n","297 0.006231972947716713\n","298 0.005922640208154917\n","299 0.005630125291645527\n","300 0.005353586748242378\n","301 0.0050936429761350155\n","302 0.004844441544264555\n","303 0.004609027877449989\n","304 0.004385812673717737\n","305 0.004177444614470005\n","306 0.003977971617132425\n","307 0.0037896421272307634\n","308 0.003606652608141303\n","309 0.0034362657461315393\n","310 0.003269952954724431\n","311 0.0031183159444481134\n","312 0.002973453374579549\n","313 0.0028366115875542164\n","314 0.0027064953465014696\n","315 0.0025788138154894114\n","316 0.0024594455026090145\n","317 0.002348040696233511\n","318 0.0022428478114306927\n","319 0.0021425446029752493\n","320 0.0020468435250222683\n","321 0.0019574202597141266\n","322 0.001868857885710895\n","323 0.001786750159226358\n","324 0.0017091288464143872\n","325 0.0016354373656213284\n","326 0.0015674520982429385\n","327 0.0014992087380960584\n","328 0.001434809877537191\n","329 0.0013750758953392506\n","330 0.001316471491008997\n","331 0.0012643308145925403\n","332 0.0012122683692723513\n","333 0.0011633462272584438\n","334 0.001116995932534337\n","335 0.0010725309839472175\n","336 0.0010288400808349252\n","337 0.0009881462901830673\n","338 0.000949775567278266\n","339 0.0009117128211073577\n","340 0.0008759971242398024\n","341 0.0008428632281720638\n","342 0.0008101383573375642\n","343 0.000778250687289983\n","344 0.0007487492985092103\n","345 0.0007212627097032964\n","346 0.0006938841543160379\n","347 0.0006696169730275869\n","348 0.0006454926333390176\n","349 0.0006216223118826747\n","350 0.0005997053231112659\n","351 0.0005776676116511226\n","352 0.0005572595982812345\n","353 0.0005385268596000969\n","354 0.0005200550076551735\n","355 0.0005021966062486172\n","356 0.0004848929529543966\n","357 0.00046811241190880537\n","358 0.0004533221072051674\n","359 0.00043809087947010994\n","360 0.0004237417015247047\n","361 0.0004094413889106363\n","362 0.00039695960003882647\n","363 0.0003847413754556328\n","364 0.0003723063855431974\n","365 0.0003614723973441869\n","366 0.0003493119147606194\n","367 0.0003392765065655112\n","368 0.00032861187355592847\n","369 0.0003187991678714752\n","370 0.00030944772879593074\n","371 0.00030074198730289936\n","372 0.0002918673271778971\n","373 0.0002823969698511064\n","374 0.00027467828476801515\n","375 0.00026692976825870574\n","376 0.00025877245934680104\n","377 0.0002524337905924767\n","378 0.00024528117501176894\n","379 0.00023836131731513888\n","380 0.0002314738230779767\n","381 0.0002259521425003186\n","382 0.00021940164151601493\n","383 0.00021320529049262404\n","384 0.00020831101574003696\n","385 0.0002034508070209995\n","386 0.0001976365747395903\n","387 0.00019224037532694638\n","388 0.00018728685972746462\n","389 0.00018319900846108794\n","390 0.000178304297151044\n","391 0.0001742713648127392\n","392 0.0001693726662779227\n","393 0.00016517052426934242\n","394 0.00016050064004957676\n","395 0.00015718501526862383\n","396 0.00015333642659243196\n","397 0.0001493698509875685\n","398 0.00014594662934541702\n","399 0.000142658973345533\n","400 0.00013985452824272215\n","401 0.00013649546599481255\n","402 0.0001332829415332526\n","403 0.00013075489550828934\n","404 0.0001276619004784152\n","405 0.00012482458259910345\n","406 0.00012185763625893742\n","407 0.00011958315735682845\n","408 0.00011726660886779428\n","409 0.00011465387797215953\n","410 0.00011201091547263786\n","411 0.00010988031863234937\n","412 0.00010754160030046478\n","413 0.00010541934898355976\n","414 0.00010286338510923088\n","415 0.00010037884203484282\n","416 9.855943790171295e-05\n","417 9.65180151979439e-05\n","418 9.489702642895281e-05\n","419 9.284297266276553e-05\n","420 9.092309483094141e-05\n","421 8.893836638890207e-05\n","422 8.729339606361464e-05\n","423 8.552139479434118e-05\n","424 8.404503023484722e-05\n","425 8.270566468127072e-05\n","426 8.112756040645763e-05\n","427 7.94868974480778e-05\n","428 7.82305360189639e-05\n","429 7.682271098019555e-05\n","430 7.521539373556152e-05\n","431 7.394448039121926e-05\n","432 7.243979052873328e-05\n","433 7.127083517843857e-05\n","434 7.007380190771073e-05\n","435 6.913499964866787e-05\n","436 6.771902553737164e-05\n","437 6.639068305958062e-05\n","438 6.536655564559624e-05\n","439 6.411281356122345e-05\n","440 6.29252172075212e-05\n","441 6.204292003531009e-05\n","442 6.0949074395466596e-05\n","443 5.9915993915637955e-05\n","444 5.914604480494745e-05\n","445 5.816834163852036e-05\n","446 5.7435681810602546e-05\n","447 5.6580331147415563e-05\n","448 5.544466694118455e-05\n","449 5.462696208269335e-05\n","450 5.396196138462983e-05\n","451 5.305650847731158e-05\n","452 5.202165630180389e-05\n","453 5.111983773531392e-05\n","454 5.035739013692364e-05\n","455 4.967537824995816e-05\n","456 4.912160147796385e-05\n","457 4.844342765863985e-05\n","458 4.7761528549017385e-05\n","459 4.709104905487038e-05\n","460 4.635703589883633e-05\n","461 4.554491533781402e-05\n","462 4.4976935896556824e-05\n","463 4.426477971719578e-05\n","464 4.346363493823446e-05\n","465 4.270580393495038e-05\n","466 4.2105817556148395e-05\n","467 4.1672206862131134e-05\n","468 4.115066985832527e-05\n","469 4.052368240081705e-05\n","470 3.985695730079897e-05\n","471 3.9423161069862545e-05\n","472 3.881759039359167e-05\n","473 3.8263038732111454e-05\n","474 3.7954028812237084e-05\n","475 3.729605668922886e-05\n","476 3.705267226905562e-05\n","477 3.6526311305351555e-05\n","478 3.617251059040427e-05\n","479 3.5699878935702145e-05\n","480 3.519720121403225e-05\n","481 3.469601506367326e-05\n","482 3.409709097468294e-05\n","483 3.362510688020848e-05\n","484 3.329312312416732e-05\n","485 3.2918327633524314e-05\n","486 3.25501459883526e-05\n","487 3.212203228031285e-05\n","488 3.174445009790361e-05\n","489 3.13296404783614e-05\n","490 3.096750515396707e-05\n","491 3.0463172151939943e-05\n","492 3.0166727810865268e-05\n","493 2.981447869387921e-05\n","494 2.931271228590049e-05\n","495 2.9050372177152894e-05\n","496 2.873335142794531e-05\n","497 2.844434857252054e-05\n","498 2.811675949487835e-05\n","499 2.7772206522058696e-05\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4blVbW3KvY_","executionInfo":{"status":"ok","timestamp":1632728064573,"user_tz":-480,"elapsed":9989,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}},"outputId":"9571af39-0531-4c93-b2e6-7c54fdb89aa9"},"source":["#3.2 PyTorch：定义新的自动求导函数\n","import torch\n","\n","class MyReLU(torch.autograd.Function):\n","    \"\"\"\n","    我们可以通过建立torch.autograd的子类来实现我们自定义的autograd函数，\n","    并完成张量的正向和反向传播。\n","    \"\"\"\n","    @staticmethod\n","    def forward(ctx, x):\n","        \"\"\"\n","        在正向传播中，我们接收到一个上下文对象和一个包含输入的张量；\n","        我们必须返回一个包含输出的张量，\n","        并且我们可以使用上下文对象来缓存对象，以便在反向传播中使用。\n","        \"\"\"\n","        ctx.save_for_backward(x)\n","        return x.clamp(min=0)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        \"\"\"\n","        在反向传播中，我们接收到上下文对象和一个张量，\n","        其包含了相对于正向传播过程中产生的输出的损失的梯度。\n","        我们可以从上下文对象中检索缓存的数据，\n","        并且必须计算并返回与正向传播的输入相关的损失的梯度。\n","        \"\"\"\n","        x, = ctx.saved_tensors\n","        grad_x = grad_output.clone()\n","        grad_x[x < 0] = 0\n","        return grad_x\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# N是批大小； D_in 是输入维度；\n","# H 是隐藏层维度； D_out 是输出维度\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 产生输入和输出的随机张量\n","x = torch.randn(N, D_in, device=device)\n","y = torch.randn(N, D_out, device=device)\n","\n","# 产生随机权重的张量\n","w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n","w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n","\n","learning_rate = 1e-6\n","for t in range(500):\n","    # 正向传播：使用张量上的操作来计算输出值y；\n","    # 我们通过调用 MyReLU.apply 函数来使用自定义的ReLU\n","    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n","\n","    # 计算并输出loss\n","    loss = (y_pred - y).pow(2).sum()\n","    print(t, loss.item())\n","\n","    # 使用autograd计算反向传播过程。\n","    loss.backward()\n","\n","    with torch.no_grad():\n","        # 用梯度下降更新权重\n","        w1 -= learning_rate * w1.grad\n","        w2 -= learning_rate * w2.grad\n","\n","        # 在反向传播之后手动清零梯度\n","        w1.grad.zero_()\n","        w2.grad.zero_()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0 40223792.0\n","1 40495408.0\n","2 43036764.0\n","3 38972340.0\n","4 26903232.0\n","5 14057944.0\n","6 6454536.0\n","7 3184521.0\n","8 1901739.875\n","9 1341951.75\n","10 1043388.9375\n","11 850676.625\n","12 709682.125\n","13 599990.5\n","14 511843.46875\n","15 439872.3125\n","16 380425.71875\n","17 330896.8125\n","18 289249.75\n","19 254027.0\n","20 224060.40625\n","21 198402.78125\n","22 176347.578125\n","23 157281.5\n","24 140731.484375\n","25 126299.296875\n","26 113661.53125\n","27 102548.9765625\n","28 92766.734375\n","29 84121.96875\n","30 76450.4375\n","31 69619.8046875\n","32 63521.6796875\n","33 58059.32421875\n","34 53157.109375\n","35 48749.7734375\n","36 44775.3671875\n","37 41183.73046875\n","38 37932.3515625\n","39 34983.359375\n","40 32304.64453125\n","41 29866.541015625\n","42 27642.2109375\n","43 25609.1171875\n","44 23748.5625\n","45 22043.015625\n","46 20478.130859375\n","47 19039.87890625\n","48 17716.828125\n","49 16497.39453125\n","50 15372.681640625\n","51 14334.2294921875\n","52 13374.41796875\n","53 12486.8564453125\n","54 11665.103515625\n","55 10903.5654296875\n","56 10197.15234375\n","57 9540.9970703125\n","58 8931.451171875\n","59 8364.6796875\n","60 7837.0693359375\n","61 7345.7705078125\n","62 6888.083984375\n","63 6461.42041015625\n","64 6063.19873046875\n","65 5691.5341796875\n","66 5344.619140625\n","67 5020.58154296875\n","68 4717.5537109375\n","69 4434.130859375\n","70 4168.9501953125\n","71 3920.64404296875\n","72 3688.14453125\n","73 3470.267822265625\n","74 3266.054443359375\n","75 3074.53759765625\n","76 2894.919189453125\n","77 2726.447021484375\n","78 2568.312255859375\n","79 2419.829833984375\n","80 2280.372802734375\n","81 2149.318603515625\n","82 2026.181884765625\n","83 1910.4132080078125\n","84 1801.5908203125\n","85 1699.232177734375\n","86 1602.9300537109375\n","87 1512.31689453125\n","88 1427.04345703125\n","89 1346.7958984375\n","90 1271.2332763671875\n","91 1200.066650390625\n","92 1133.02392578125\n","93 1069.8577880859375\n","94 1010.36376953125\n","95 954.2825317382812\n","96 901.4044799804688\n","97 851.5562133789062\n","98 804.552734375\n","99 760.2352294921875\n","100 718.4368286132812\n","101 678.9876708984375\n","102 641.7749633789062\n","103 606.6571655273438\n","104 573.5272827148438\n","105 542.2532958984375\n","106 512.7191162109375\n","107 484.84326171875\n","108 458.51220703125\n","109 433.6602478027344\n","110 410.1883850097656\n","111 388.00799560546875\n","112 367.0562744140625\n","113 347.2635498046875\n","114 328.5697021484375\n","115 310.89739990234375\n","116 294.1966552734375\n","117 278.40966796875\n","118 263.49090576171875\n","119 249.38600158691406\n","120 236.05099487304688\n","121 223.44395446777344\n","122 211.52438354492188\n","123 200.24945068359375\n","124 189.59243774414062\n","125 179.51113891601562\n","126 169.97427368164062\n","127 160.95252990722656\n","128 152.4157257080078\n","129 144.3421630859375\n","130 136.70318603515625\n","131 129.47314453125\n","132 122.63372802734375\n","133 116.16136169433594\n","134 110.03643035888672\n","135 104.23516845703125\n","136 98.74798583984375\n","137 93.55704498291016\n","138 88.6443099975586\n","139 83.99467468261719\n","140 79.59187316894531\n","141 75.42376708984375\n","142 71.47875213623047\n","143 67.74407958984375\n","144 64.20457458496094\n","145 60.85407638549805\n","146 57.67815399169922\n","147 54.672149658203125\n","148 51.82789611816406\n","149 49.13147735595703\n","150 46.57712936401367\n","151 44.15771484375\n","152 41.86534881591797\n","153 39.69535827636719\n","154 37.63606262207031\n","155 35.68437957763672\n","156 33.83652114868164\n","157 32.08755111694336\n","158 30.426525115966797\n","159 28.854684829711914\n","160 27.362384796142578\n","161 25.949268341064453\n","162 24.611007690429688\n","163 23.34139633178711\n","164 22.136146545410156\n","165 20.9960994720459\n","166 19.914505004882812\n","167 18.889589309692383\n","168 17.91714096069336\n","169 16.995691299438477\n","170 16.121784210205078\n","171 15.293731689453125\n","172 14.507342338562012\n","173 13.763136863708496\n","174 13.057365417480469\n","175 12.387072563171387\n","176 11.752054214477539\n","177 11.14941692352295\n","178 10.578401565551758\n","179 10.036371231079102\n","180 9.52270793914795\n","181 9.035931587219238\n","182 8.573319435119629\n","183 8.135257720947266\n","184 7.719637393951416\n","185 7.325125217437744\n","186 6.950944423675537\n","187 6.595880508422852\n","188 6.259488105773926\n","189 5.939967155456543\n","190 5.636471748352051\n","191 5.349099159240723\n","192 5.07637882232666\n","193 4.8172454833984375\n","194 4.572361469268799\n","195 4.339512348175049\n","196 4.118614196777344\n","197 3.9088072776794434\n","198 3.710111618041992\n","199 3.5213751792907715\n","200 3.3427252769470215\n","201 3.1724512577056885\n","202 3.01090669631958\n","203 2.858226776123047\n","204 2.712862968444824\n","205 2.575042247772217\n","206 2.444685459136963\n","207 2.3205389976501465\n","208 2.2027580738067627\n","209 2.091153621673584\n","210 1.985133409500122\n","211 1.8845164775848389\n","212 1.7886308431625366\n","213 1.698152780532837\n","214 1.6122190952301025\n","215 1.5305423736572266\n","216 1.4530980587005615\n","217 1.3796082735061646\n","218 1.3095988035202026\n","219 1.2433788776397705\n","220 1.1806222200393677\n","221 1.120766282081604\n","222 1.0640969276428223\n","223 1.0102728605270386\n","224 0.9593685865402222\n","225 0.910703182220459\n","226 0.8646633625030518\n","227 0.820992648601532\n","228 0.7795892953872681\n","229 0.7401283979415894\n","230 0.702708899974823\n","231 0.6673388481140137\n","232 0.6336250305175781\n","233 0.6016658544540405\n","234 0.5714210271835327\n","235 0.5425202250480652\n","236 0.5152062177658081\n","237 0.4892319142818451\n","238 0.4645296633243561\n","239 0.4411082863807678\n","240 0.4189014434814453\n","241 0.3978869318962097\n","242 0.37770962715148926\n","243 0.3587292432785034\n","244 0.34073883295059204\n","245 0.323599249124527\n","246 0.30728501081466675\n","247 0.29184579849243164\n","248 0.2770705819129944\n","249 0.2631654739379883\n","250 0.2499968260526657\n","251 0.23744916915893555\n","252 0.2254464328289032\n","253 0.21424883604049683\n","254 0.20342889428138733\n","255 0.19324497878551483\n","256 0.18352243304252625\n","257 0.17423832416534424\n","258 0.16546879708766937\n","259 0.1571497917175293\n","260 0.1492491066455841\n","261 0.14180994033813477\n","262 0.13470891118049622\n","263 0.12788888812065125\n","264 0.12151387333869934\n","265 0.11541888862848282\n","266 0.10961014777421951\n","267 0.10409115254878998\n","268 0.09885700047016144\n","269 0.09393823146820068\n","270 0.08923733234405518\n","271 0.08472840487957001\n","272 0.08046391606330872\n","273 0.07642368972301483\n","274 0.07263720780611038\n","275 0.06896790117025375\n","276 0.06550309807062149\n","277 0.06222721189260483\n","278 0.059118177741765976\n","279 0.05613888427615166\n","280 0.05335560068488121\n","281 0.05066616088151932\n","282 0.04816148430109024\n","283 0.04576971009373665\n","284 0.04347296804189682\n","285 0.04129215329885483\n","286 0.039243802428245544\n","287 0.03726918250322342\n","288 0.03542362153530121\n","289 0.033665068447589874\n","290 0.031975794583559036\n","291 0.030387628823518753\n","292 0.028903044760227203\n","293 0.027451353147625923\n","294 0.026079095900058746\n","295 0.024779895320534706\n","296 0.023545969277620316\n","297 0.02238529548048973\n","298 0.021275851875543594\n","299 0.020218228921294212\n","300 0.019203316420316696\n","301 0.018257340416312218\n","302 0.01734749786555767\n","303 0.016484148800373077\n","304 0.01567012630403042\n","305 0.014901747927069664\n","306 0.014163317158818245\n","307 0.01347002200782299\n","308 0.012790891341865063\n","309 0.012163360603153706\n","310 0.011584825813770294\n","311 0.011019742116332054\n","312 0.010479381307959557\n","313 0.009961124509572983\n","314 0.009489677846431732\n","315 0.009031789377331734\n","316 0.008581907488405704\n","317 0.008164101280272007\n","318 0.007761267013847828\n","319 0.007391814142465591\n","320 0.0070327757857739925\n","321 0.006692766677588224\n","322 0.006382680963724852\n","323 0.006076434627175331\n","324 0.005794689990580082\n","325 0.0055117858573794365\n","326 0.0052600097842514515\n","327 0.0050132013857364655\n","328 0.0047725229524075985\n","329 0.004552947357296944\n","330 0.004334283992648125\n","331 0.004137558862566948\n","332 0.0039446232840418816\n","333 0.003766363486647606\n","334 0.0035893660970032215\n","335 0.0034217240754514933\n","336 0.0032688872888684273\n","337 0.0031167990528047085\n","338 0.002977205440402031\n","339 0.002842556219547987\n","340 0.0027176709845662117\n","341 0.0025933673605322838\n","342 0.002479992574080825\n","343 0.002371596172451973\n","344 0.002266387455165386\n","345 0.0021668453700840473\n","346 0.0020721978507936\n","347 0.0019804330077022314\n","348 0.0018960799789056182\n","349 0.001817135838791728\n","350 0.0017381820362061262\n","351 0.0016641849651932716\n","352 0.0015936284326016903\n","353 0.001530028530396521\n","354 0.0014636283740401268\n","355 0.0014060796238481998\n","356 0.0013500757049769163\n","357 0.001296846428886056\n","358 0.001245132414624095\n","359 0.001196965342387557\n","360 0.0011520988773554564\n","361 0.0011090801563113928\n","362 0.0010636040242388844\n","363 0.0010220418917015195\n","364 0.000982107361778617\n","365 0.0009467006893828511\n","366 0.0009120363974943757\n","367 0.0008785785757936537\n","368 0.0008459761156700552\n","369 0.0008132468210533261\n","370 0.0007857752498239279\n","371 0.0007569072768092155\n","372 0.0007279515848495066\n","373 0.0007039652555249631\n","374 0.0006796165253035724\n","375 0.0006554188439622521\n","376 0.0006317334482446313\n","377 0.0006109982496127486\n","378 0.0005881587276235223\n","379 0.0005692413542419672\n","380 0.0005511161871254444\n","381 0.0005324634839780629\n","382 0.0005149298231117427\n","383 0.0004974360927008092\n","384 0.00048248283565044403\n","385 0.00046618367196060717\n","386 0.0004507907433435321\n","387 0.0004378783341962844\n","388 0.00042437438969500363\n","389 0.0004104207328055054\n","390 0.0003980898472946137\n","391 0.0003857205156236887\n","392 0.0003748271847143769\n","393 0.00036317718331702054\n","394 0.0003517466248013079\n","395 0.0003423152957111597\n","396 0.00033267756225541234\n","397 0.0003240058431401849\n","398 0.0003136522718705237\n","399 0.00030475063249468803\n","400 0.0002961728605441749\n","401 0.00028725358424708247\n","402 0.0002800647635012865\n","403 0.0002724032965488732\n","404 0.00026440637884661555\n","405 0.0002564888563938439\n","406 0.0002499883412383497\n","407 0.00024290633155032992\n","408 0.0002363279345445335\n","409 0.0002304735971847549\n","410 0.0002242091577500105\n","411 0.00021912586817052215\n","412 0.00021294508769642562\n","413 0.00020726004731841385\n","414 0.00020244769984856248\n","415 0.0001973641337826848\n","416 0.00019192301260773093\n","417 0.0001867619139375165\n","418 0.00018274356261827052\n","419 0.00017885034321807325\n","420 0.0001742814783938229\n","421 0.00017080092220567167\n","422 0.00016657350352033973\n","423 0.00016299137496389449\n","424 0.00015889888163655996\n","425 0.00015539438754785806\n","426 0.00015170470578595996\n","427 0.00014789383567404002\n","428 0.0001450727868359536\n","429 0.00014143362932372838\n","430 0.00013855172437615693\n","431 0.0001352196850348264\n","432 0.00013179672532714903\n","433 0.00012911975500173867\n","434 0.0001260758435819298\n","435 0.00012362476263660938\n","436 0.00012114793935324997\n","437 0.00011887979053426534\n","438 0.00011657258437480778\n","439 0.00011430616723373532\n","440 0.00011183833703398705\n","441 0.00010965351248160005\n","442 0.00010718054545577615\n","443 0.00010528745769988745\n","444 0.00010287664190400392\n","445 0.00010081893560709432\n","446 9.905242768581957e-05\n","447 9.738076914800331e-05\n","448 9.569026587996632e-05\n","449 9.368004248244688e-05\n","450 9.189263801090419e-05\n","451 9.020130528369918e-05\n","452 8.850933227222413e-05\n","453 8.688327216077596e-05\n","454 8.528338366886601e-05\n","455 8.37384577607736e-05\n","456 8.23626178316772e-05\n","457 8.040051034186035e-05\n","458 7.897035538917407e-05\n","459 7.791106327204034e-05\n","460 7.669004844501615e-05\n","461 7.54773645894602e-05\n","462 7.395961438305676e-05\n","463 7.268103945534676e-05\n","464 7.136742351576686e-05\n","465 7.027409446891397e-05\n","466 6.937076977919787e-05\n","467 6.808388570789248e-05\n","468 6.695725460303947e-05\n","469 6.621707871090621e-05\n","470 6.51052687317133e-05\n","471 6.383148866007105e-05\n","472 6.322024273686111e-05\n","473 6.179716729093343e-05\n","474 6.0917125665582716e-05\n","475 5.996832260279916e-05\n","476 5.906604928895831e-05\n","477 5.814078758703545e-05\n","478 5.743731162510812e-05\n","479 5.666927609127015e-05\n","480 5.580191646004096e-05\n","481 5.496972516993992e-05\n","482 5.4333788284566253e-05\n","483 5.319175761542283e-05\n","484 5.2615374443121254e-05\n","485 5.2003320888616145e-05\n","486 5.102650902699679e-05\n","487 5.04314957652241e-05\n","488 4.9493137339595705e-05\n","489 4.891276330454275e-05\n","490 4.794952110387385e-05\n","491 4.7517140046693385e-05\n","492 4.6810593630652875e-05\n","493 4.6334123908309266e-05\n","494 4.571209501591511e-05\n","495 4.5269509428180754e-05\n","496 4.460975469555706e-05\n","497 4.413703572936356e-05\n","498 4.3426167394500226e-05\n","499 4.2768871935550123e-05\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"dgJfG9CEKzP3","executionInfo":{"status":"error","timestamp":1632728066718,"user_tz":-480,"elapsed":1934,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}},"outputId":"03ab6a7e-a649-49c1-df3c-bc485983b3bc"},"source":["#3.3 TensorFlow：静态图\n","import tensorflow as tf\n","import numpy as np\n","\n","# 首先我们建立计算图（computational graph）\n","\n","# N是批大小；D是输入维度；\n","# H是隐藏层维度；D_out是输出维度。\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 为输入和目标数据创建placeholder；\n","# 当执行计算图时，他们将会被真实的数据填充\n","x = tf.placeholder(tf.float32, shape=(None, D_in))\n","y = tf.placeholder(tf.float32, shape=(None, D_out))\n","\n","# 为权重创建Variable并用随机数据初始化\n","# TensorFlow的Variable在执行计算图时不会改变\n","w1 = tf.Variable(tf.random_normal((D_in, H)))\n","w2 = tf.Variable(tf.random_normal((H, D_out)))\n","\n","# 前向传播：使用TensorFlow的张量运算计算预测值y。\n","# 注意这段代码实际上不执行任何数值运算；\n","# 它只是建立了我们稍后将执行的计算图。\n","h = tf.matmul(x, w1)\n","h_relu = tf.maximum(h, tf.zeros(1))\n","y_pred = tf.matmul(h_relu, w2)\n","\n","# 使用TensorFlow的张量运算损失（loss）\n","loss = tf.reduce_sum((y - y_pred) ** 2.0)\n","\n","# 计算loss对于w1和w2的导数\n","grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n","\n","# 使用梯度下降更新权重。为了实际更新权重，我们需要在执行计算图时计算new_w1和new_w2。\n","# 注意，在TensorFlow中，更新权重值的行为是计算图的一部分;\n","# 但在PyTorch中，这发生在计算图形之外。\n","learning_rate = 1e-6\n","new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n","new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n","\n","# 现在我们搭建好了计算图，所以我们开始一个TensorFlow的会话（session）来实际执行计算图。\n","with tf.Session() as sess:\n","\n","    # 运行一次计算图来初始化Variable w1和w2\n","    sess.run(tf.global_variables_initializer())\n","\n","    # 创建numpy数组来存储输入x和目标y的实际数据\n","    x_value = np.random.randn(N, D_in)\n","    y_value = np.random.randn(N, D_out)\n","\n","    for _ in range(500):\n","        # 多次运行计算图。每次执行时，我们都用feed_dict参数，\n","        # 将x_value绑定到x，将y_value绑定到y，\n","        # 每次执行图形时我们都要计算损失、new_w1和new_w2；\n","        # 这些张量的值以numpy数组的形式返回。\n","        loss_value, _, _ = sess.run([loss, new_w1, new_w2], \n","                                    feed_dict={x: x_value, y: y_value})\n","        print(loss_value)"],"execution_count":5,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e5329c1aa6fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 为输入和目标数据创建placeholder；\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 当执行计算图时，他们将会被真实的数据填充\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"]}]},{"cell_type":"code","metadata":{"id":"AuFAhZ0wK4kP","executionInfo":{"status":"aborted","timestamp":1632728066716,"user_tz":-480,"elapsed":5,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}}},"source":["#4.nn模块\n","#4.1 PyTorch：nn\n","# -*- coding: utf-8 -*-\n","import torch\n","\n","# N是批大小；D是输入维度\n","# H是隐藏层维度；D_out是输出维度\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","#创建输入和输出随机张量\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","\n","# 使用nn包将我们的模型定义为一系列的层。\n","# nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出。\n","# 每个线性模块使用线性函数从输入计算输出，并保存其内部的权重和偏差张量。\n","# 在构造模型之后，我们使用.to()方法将其移动到所需的设备。\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(D_in, H),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(H, D_out),\n",")\n","\n","# nn包还包含常用的损失函数的定义；\n","# 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数。\n","# 设置reduction='sum'，表示我们计算的是平方误差的“和”，而不是平均值;\n","# 这是为了与前面我们手工计算损失的例子保持一致，\n","# 但是在实践中，通过设置reduction='elementwise_mean'来使用均方误差作为损失更为常见。\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","learning_rate = 1e-4\n","for t in range(500):\n","    # 前向传播：通过向模型传入x计算预测的y。\n","    # 模块对象重载了__call__运算符，所以可以像函数那样调用它们。\n","    # 这么做相当于向模块传入了一个张量，然后它返回了一个输出张量。\n","    y_pred = model(x)\n","\n","     # 计算并打印损失。\n","     # 传递包含y的预测值和真实值的张量，损失函数返回包含损失的张量。\n","    loss = loss_fn(y_pred, y)\n","    print(t, loss.item())\n","\n","    # 反向传播之前清零梯度\n","    model.zero_grad()\n","\n","    # 反向传播：计算模型的损失对所有可学习参数的导数（梯度）。\n","    # 在内部，每个模块的参数存储在requires_grad=True的张量中，\n","    # 因此这个调用将计算模型中所有可学习参数的梯度。\n","    loss.backward()\n","\n","    # 使用梯度下降更新权重。\n","    # 每个参数都是张量，所以我们可以像我们以前那样可以得到它的数值和梯度\n","    with torch.no_grad():\n","        for param in model.parameters():\n","            param -= learning_rate * param.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2W-lXhDK7lv","executionInfo":{"status":"aborted","timestamp":1632728066716,"user_tz":-480,"elapsed":5,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}}},"source":["#4.2 PyTorch：optim\n","import torch\n","\n","# N是批大小；D是输入维度\n","# H是隐藏层维度；D_out是输出维度\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 产生随机输入和输出张量\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","\n","# 使用nn包定义模型和损失函数\n","model = torch.nn.Sequential(\n","          torch.nn.Linear(D_in, H),\n","          torch.nn.ReLU(),\n","          torch.nn.Linear(H, D_out),\n","        )\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","\n","# 使用optim包定义优化器（Optimizer）。Optimizer将会为我们更新模型的权重。\n","# 这里我们使用Adam优化方法；optim包还包含了许多别的优化算法。\n","# Adam构造函数的第一个参数告诉优化器应该更新哪些张量。\n","learning_rate = 1e-4\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","for t in range(500):\n","\n","    # 前向传播：通过像模型输入x计算预测的y\n","    y_pred = model(x)\n","\n","    # 计算并打印loss\n","    loss = loss_fn(y_pred, y)\n","    print(t, loss.item())\n","\n","    # 在反向传播之前，使用optimizer将它要更新的所有张量的梯度清零(这些张量是模型可学习的权重)\n","    optimizer.zero_grad()\n","\n","    # 反向传播：根据模型的参数计算loss的梯度\n","    loss.backward()\n","\n","    # 调用Optimizer的step函数使它所有参数更新\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWzwXv6ULCYg","executionInfo":{"status":"aborted","timestamp":1632728066717,"user_tz":-480,"elapsed":6,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}}},"source":["#4.3 PyTorch：自定义nn模块\n","import torch\n","\n","class TwoLayerNet(torch.nn.Module):\n","    def __init__(self, D_in, H, D_out):\n","        \"\"\"\n","        在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。\n","        \"\"\"\n","        super(TwoLayerNet, self).__init__()\n","        self.linear1 = torch.nn.Linear(D_in, H)\n","        self.linear2 = torch.nn.Linear(H, D_out)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。\n","        我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。\n","        \"\"\"\n","        h_relu = self.linear1(x).clamp(min=0)\n","        y_pred = self.linear2(h_relu)\n","        return y_pred\n","\n","# N是批大小； D_in 是输入维度；\n","# H 是隐藏层维度； D_out 是输出维度\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 产生输入和输出的随机张量\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","\n","# 通过实例化上面定义的类来构建我们的模型。\n","model = TwoLayerNet(D_in, H, D_out)\n","\n","# 构造损失函数和优化器。\n","# SGD构造函数中对model.parameters()的调用，\n","# 将包含模型的一部分，即两个nn.Linear模块的可学习参数。\n","loss_fn = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n","for t in range(500):\n","    # 前向传播：通过向模型传递x计算预测值y\n","    y_pred = model(x)\n","\n","    #计算并输出loss\n","    loss = loss_fn(y_pred, y)\n","    print(t, loss.item())\n","\n","    # 清零梯度，反向传播，更新权重\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uy3q0cbrLFsg","executionInfo":{"status":"aborted","timestamp":1632728066718,"user_tz":-480,"elapsed":7,"user":{"displayName":"Liao Jack","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16157886839679822522"}}},"source":["#4.4 PyTorch：控制流和权重共享\n","import random\n","import torch\n","\n","class DynamicNet(torch.nn.Module):\n","    def __init__(self, D_in, H, D_out):\n","        \"\"\"\n","        在构造函数中，我们构造了三个nn.Linear实例，它们将在前向传播时被使用。\n","        \"\"\"\n","        super(DynamicNet, self).__init__()\n","        self.input_linear = torch.nn.Linear(D_in, H)\n","        self.middle_linear = torch.nn.Linear(H, H)\n","        self.output_linear = torch.nn.Linear(H, D_out)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        对于模型的前向传播，我们随机选择0、1、2、3，\n","        并重用了多次计算隐藏层的middle_linear模块。\n","        由于每个前向传播构建一个动态计算图，\n","        我们可以在定义模型的前向传播时使用常规Python控制流运算符，如循环或条件语句。\n","        在这里，我们还看到，在定义计算图形时多次重用同一个模块是完全安全的。\n","        这是Lua Torch的一大改进，因为Lua Torch中每个模块只能使用一次。\n","        \"\"\"\n","        h_relu = self.input_linear(x).clamp(min=0)\n","        for _ in range(random.randint(0, 3)):\n","            h_relu = self.middle_linear(h_relu).clamp(min=0)\n","        y_pred = self.output_linear(h_relu)\n","        return y_pred\n","\n","\n","# N是批大小；D是输入维度\n","# H是隐藏层维度；D_out是输出维度\n","N, D_in, H, D_out = 64, 1000, 100, 10\n","\n","# 产生输入和输出随机张量\n","x = torch.randn(N, D_in)\n","y = torch.randn(N, D_out)\n","\n","# 实例化上面定义的类来构造我们的模型\n","model = DynamicNet(D_in, H, D_out)\n","\n","# 构造我们的损失函数（loss function）和优化器（Optimizer）。\n","# 用平凡的随机梯度下降训练这个奇怪的模型是困难的，所以我们使用了momentum方法。\n","criterion = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n","for t in range(500):\n","\n","    # 前向传播：通过向模型传入x计算预测的y。\n","    y_pred = model(x)\n","\n","    # 计算并打印损失\n","    loss = criterion(y_pred, y)\n","    print(t, loss.item())\n","\n","    # 清零梯度，反向传播，更新权重 \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":null,"outputs":[]}]}