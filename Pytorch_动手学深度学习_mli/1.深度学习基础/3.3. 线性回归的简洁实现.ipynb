{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.3. 线性回归的简洁实现.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM2eFRuqoZK6I7KrbmlF5/P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ZzKnh9JKRHsh"},"source":["!pip install d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"avcXEjK5DtMC"},"source":["#3.3.1. 生成数据集\n","import numpy as np\n","import torch\n","from torch.utils import data\n","from d2l import torch as d2l\n","\n","true_w = torch.tensor([2, -3.4])\n","true_b = 4.2\n","features, labels = d2l.synthetic_data(true_w, true_b, 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzbkKv8PRNwO"},"source":["#3.3.2. 读取数据集\n","#将 features 和 labels 作为API的参数传递，并在实例化数据迭代器对象时指定 batch_size。此外，布尔值 is_train 表示是否希望数据迭代器对象在每个迭代周期内打乱数据。\n","def load_array(data_arrays, batch_size, is_train=True):  #@save\n","    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n","    dataset = data.TensorDataset(*data_arrays)\n","    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","batch_size = 10\n","data_iter = load_array((features, labels), batch_size)\n","\n","next(iter(data_iter))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1_MrOJBRo0J"},"source":["#3.3.3. 定义模型\n","#在 PyTorch 中，全连接层在 Linear 类中定义。值得注意的是，我们将两个参数传递到 nn.Linear 中。第一个指定输入特征形状，即 2，第二个指定输出特征形状，输出特征形状为单个标量，因此为 1。\n","# `nn` 是神经网络的缩写\n","from torch import nn\n","\n","net = nn.Sequential(nn.Linear(2, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKQYJG1yRwJn"},"source":["#3.3.4. 初始化模型参数\n","#指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，偏置参数将初始化为零。\n","net[0].weight.data.normal_(0, 0.01)\n","net[0].bias.data.fill_(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FMeHV-mJR2gy"},"source":["#3.3.5. 定义损失函数\n","#计算均方误差使用的是MSELoss类，也称为平方  L2  范数。默认情况下，它返回所有样本损失的平均值。\n","loss = nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OXRq64KR7Sl"},"source":["#3.3.6. 定义优化算法\n","#小批量随机梯度下降算法是一种优化神经网络的标准工具，PyTorch 在 optim 模块中实现了该算法的许多变种。\n","trainer = torch.optim.SGD(net.parameters(), lr=0.03)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJIaCDC1SNxX"},"source":["#3.3.7. 训练\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    for X, y in data_iter:\n","        l = loss(net(X), y)\n","        trainer.zero_grad()\n","        l.backward()\n","        trainer.step()\n","    l = loss(net(features), labels)\n","    print(f'epoch {epoch + 1}, loss {l:f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jQur1PvzSWe8"},"source":["w = net[0].weight.data\n","print('w的估计误差：', true_w - w.reshape(true_w.shape))\n","b = net[0].bias.data\n","print('b的估计误差：', true_b - b)"],"execution_count":null,"outputs":[]}]}