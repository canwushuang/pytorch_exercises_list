{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.5. 自动求导.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOwrMW3ccog3NJXmXro0DQ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"OSxW3ULNtB5O"},"source":["#2.5.1. 一个简单的例子\n","#假设我们想对函数  y=2x⊤x 关于列向量  x 求导。首先，我们创建变量 x 并为其分配一个初始值。\n","import torch\n","\n","x = torch.arange(4.0)\n","x\n","\n","x.requires_grad_(True)  # 等价于 `x = torch.arange(4.0, requires_grad=True)`\n","print(x.grad)  # 默认值是None\n","#计算y\n","y = 2 * torch.dot(x, x)\n","y\n","#x 是一个长度为 4 的向量，计算 x 和 x 的内积，得到了我们赋值给 y 的标量输出。接下来，我们可以通过调用反向传播函数来自动计算y关于x 每个分量的梯度，并打印这些梯度。\n","y.backward()\n","x.grad\n","#函数  y=2x⊤x  关于 x  的梯度应为 4x 。让我们快速验证我们想要的梯度是否正确计算。\n","x.grad == 4 * x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nO5UJwsht3Ky"},"source":["#让我们计算 x 的另一个函数。\n","# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n","x.grad.zero_()\n","y = x.sum()\n","y.backward()\n","x.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UoV5Z_5CuODA"},"source":["#2.5.2. 非标量变量的反向传播\n","# 对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n","x.grad.zero_()\n","y = x * x\n","# 等价于y.backward(torch.ones(len(x)))\n","y.sum().backward()\n","x.grad"],"execution_count":null,"outputs":[]}]}