{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11.10. Adam.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOD9cYo/EQLnQuRcGU/uLrv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"E55dBwffj83w"},"source":["#11.10.2. 实施\n","#从头开始实施亚当并不是很艰巨。为方便起见，我们将时间步长计数器 t 存储在 hyperparams 字典中。除此之外，一切都很简单。\n","!pip install d2l\n","%matplotlib inline\n","import torch\n","from d2l import torch as d2l\n","\n","\n","def init_adam_states(feature_dim):\n","    v_w, v_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n","    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n","    return ((v_w, s_w), (v_b, s_b))\n","\n","def adam(params, states, hyperparams):\n","    beta1, beta2, eps = 0.9, 0.999, 1e-6\n","    for p, (v, s) in zip(params, states):\n","        with torch.no_grad():\n","            v[:] = beta1 * v + (1 - beta1) * p.grad\n","            s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)\n","            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n","            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n","            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)\n","                                                       + eps)\n","        p.grad.data.zero_()\n","    hyperparams['t'] += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"myjI2QM4kFKl"},"source":["#我们准备好用亚当来训练模型了。我们使用  η=0.01  的学习率。\n","data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n","d2l.train_ch11(adam, init_adam_states(feature_dim),\n","               {'lr': 0.01, 't': 1}, data_iter, feature_dim);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKFP8sowkKVk"},"source":["#更简洁的实现非常简单，因为 adam 是作为 Gluon trainer 优化库的一部分提供的算法之一。\n","#因此，我们只需要为 Gluon 中的实现传递配置参数。\n","trainer = torch.optim.Adam\n","d2l.train_concise_ch11(trainer, {'lr': 0.01}, data_iter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWDna1wJkPcl"},"source":["#11.10.3. 瑜珈修行者\n","def yogi(params, states, hyperparams):\n","    beta1, beta2, eps = 0.9, 0.999, 1e-3\n","    for p, (v, s) in zip(params, states):\n","        with torch.no_grad():\n","            v[:] = beta1 * v + (1 - beta1) * p.grad\n","            s[:] = s + (1 - beta2) * torch.sign(\n","                torch.square(p.grad) - s) * torch.square(p.grad)\n","            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n","            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n","            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)\n","                                                       + eps)\n","        p.grad.data.zero_()\n","    hyperparams['t'] += 1\n","\n","data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n","d2l.train_ch11(yogi, init_adam_states(feature_dim),\n","               {'lr': 0.01, 't': 1}, data_iter, feature_dim);"],"execution_count":null,"outputs":[]}]}