{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11.11. 学习率排定.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfVq8o7QWFYMoPPGjiDglF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"FQ1mYSYOkXT6"},"source":["#11.11.1. 玩具问题\n","!pip install d2l\n","%matplotlib inline\n","import math\n","import torch\n","from torch import nn\n","from torch.optim import lr_scheduler\n","from d2l import torch as d2l\n","\n","\n","def net_fn():\n","    model = nn.Sequential(\n","        nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","        nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","        nn.Flatten(),\n","        nn.Linear(16 * 5 * 5, 120), nn.ReLU(),\n","        nn.Linear(120, 84), nn.ReLU(),\n","        nn.Linear(84, 10))\n","\n","    return model\n","\n","loss = nn.CrossEntropyLoss()\n","device = d2l.try_gpu()\n","\n","batch_size = 256\n","train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n","\n","# The code is almost identical to `d2l.train_ch6` defined in the\n","# lenet section of chapter convolutional neural networks\n","def train(net, train_iter, test_iter, num_epochs, loss, trainer, device,\n","          scheduler=None):\n","    net.to(device)\n","    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],\n","                            legend=['train loss', 'train acc', 'test acc'])\n","\n","    for epoch in range(num_epochs):\n","        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n","        for i, (X, y) in enumerate(train_iter):\n","            net.train()\n","            trainer.zero_grad()\n","            X, y = X.to(device), y.to(device)\n","            y_hat = net(X)\n","            l = loss(y_hat, y)\n","            l.backward()\n","            trainer.step()\n","            with torch.no_grad():\n","                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n","            train_loss = metric[0] / metric[2]\n","            train_acc = metric[1] / metric[2]\n","            if (i + 1) % 50 == 0:\n","                animator.add(epoch + i / len(train_iter),\n","                             (train_loss, train_acc, None))\n","\n","        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n","        animator.add(epoch+1, (None, None, test_acc))\n","\n","        if scheduler:\n","            if scheduler.__module__ == lr_scheduler.__name__:\n","                # Using PyTorch In-Built scheduler\n","                scheduler.step()\n","            else:\n","                # Using custom defined scheduler\n","                for param_group in trainer.param_groups:\n","                    param_group['lr'] = scheduler(epoch)\n","\n","    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n","          f'test acc {test_acc:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fn3jQus3kgNO"},"source":["#让我们来看看如果我们使用默认设置调用此算法，例如学习率为  0.3  并训练  30  次迭代，会发生什么情况。\n","#请注意，在测试准确度方面的进展停滞超过一点时，训练准确度如何持续提高。两条曲线之间的间隙表示过度拟合。\n","lr, num_epochs = 0.3, 30\n","net = net_fn()\n","trainer = torch.optim.SGD(net.parameters(), lr=lr)\n","train(net, train_iter, test_iter, num_epochs, loss, trainer, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"guMrSmlbklG1"},"source":["#11.11.2. 调度程序\n","#调整学习率的一种方法是在每一步明确设置学习率。这可以通过 set_learning_rate 方法方便地实现。\n","#我们可以在每个时代之后（甚至在每个迷你批次之后）向下调整它，例如，以动态的方式来响应优化的进展情况。\n","lr = 0.1\n","trainer.param_groups[0][\"lr\"] = lr\n","print(f'learning rate is now {trainer.param_groups[0][\"lr\"]:.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"021m-wrdkpac"},"source":["class SquareRootScheduler:\n","    def __init__(self, lr=0.1):\n","        self.lr = lr\n","\n","    def __call__(self, num_update):\n","        return self.lr * pow(num_update + 1.0, -0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPbYM0KokrZ8"},"source":["#让我们在一系列值上绘制它的行为。\n","scheduler = SquareRootScheduler(lr=0.1)\n","d2l.plot(torch.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4R29FD01kvzl"},"source":["#现在让我们来看看这对 Fashion-MNIST 的训练有何影响。我们只是提供调度程序作为训练算法的额外参数。\n","net = net_fn()\n","trainer = torch.optim.SGD(net.parameters(), lr)\n","train(net, train_iter, test_iter, num_epochs, loss, trainer, device,\n","      scheduler)"],"execution_count":null,"outputs":[]}]}