{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11.6. 动量法.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNXmR4+KuyMkDu5TBwrH3P6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"CCHDlAZyIAYr"},"source":["#11.6.1.2. 条件不佳的问题\n","!pip install d2l\n","%matplotlib inline\n","import torch\n","from d2l import torch as d2l\n","\n","eta = 0.4\n","def f_2d(x1, x2):\n","    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n","def gd_2d(x1, x2, s1, s2):\n","    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n","\n","d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D7S5oT1_IIbK"},"source":["eta = 0.6\n","d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFAbmiFZII-Z"},"source":["#11.6.1.3. 动量方法\n","#请注意，对于  β=0 ，我们恢复常规梯度下降。在深入研究数学属性之前，让我们快速看一下算法在实践中的行为方式。\n","def momentum_2d(x1, x2, v1, v2):\n","    v1 = beta * v1 + 0.2 * x1\n","    v2 = beta * v2 + 4 * x2\n","    return x1 - eta * v1, x2 - eta * v2, v1, v2\n","\n","eta, beta = 0.6, 0.5\n","d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-ExBxgdIPdm"},"source":["#正如我们所看到的那样，即使我们以前使用的学习率相同，势头仍然很好地收敛。让我们看看当我们降低动量参数时会发生什么。\n","#将其减半至  β=0.25  会导致一条几乎没有收敛的轨迹。\n","#尽管如此，它比没有动力要好得多（当解决方案分歧时）。\n","eta, beta = 0.6, 0.25\n","d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fN0yp1O8IVSm"},"source":["#11.6.1.4. 有效样品重量\n","d2l.set_figsize()\n","betas = [0.95, 0.9, 0.6, 0]\n","for beta in betas:\n","    x = torch.arange(40).detach().numpy()\n","    d2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')\n","d2l.plt.xlabel('time')\n","d2l.plt.legend();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"953SWWVrIYqW"},"source":["#11.6.2.1. 从头开始实施\n","#与（迷你匹配）随机梯度下降相比，动量方法需要维持一组辅助变量，即速度。\n","#它与梯度（以及优化问题的变量）具有相同的形状。在下面的实施中，我们称之为这些变量 states。\n","def init_momentum_states(feature_dim):\n","    v_w = torch.zeros((feature_dim, 1))\n","    v_b = torch.zeros(1)\n","    return (v_w, v_b)\n","\n","def sgd_momentum(params, states, hyperparams):\n","    for p, v in zip(params, states):\n","        with torch.no_grad():\n","            v[:] = hyperparams['momentum'] * v + p.grad\n","            p[:] -= hyperparams['lr'] * v\n","        p.grad.data.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VLlXhjyImib"},"source":["#让我们看看这在实践中是如何运作的。\n","def train_momentum(lr, momentum, num_epochs=2):\n","    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),\n","                   {'lr': lr, 'momentum': momentum}, data_iter,\n","                   feature_dim, num_epochs)\n","\n","data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n","train_momentum(0.02, 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFUSjwBpIr83"},"source":["train_momentum(0.01, 0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKUyqTTZI33P"},"source":["#降低学习率进一步解决了任何非平滑优化问题的问题。将其设置为  0.005  会产生良好的收敛性能。\n","train_momentum(0.005, 0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5p5DHkN_I7bN"},"source":["#11.6.2.2. 简洁的实施\n","#自从标准 sgd 求解器已经建立了势头以来，在 Gluon 没什么可做的。设置匹配参数会产生非常类似的轨迹。\n","trainer = torch.optim.SGD\n","d2l.train_concise_ch11(trainer, {'lr': 0.005, 'momentum': 0.9}, data_iter)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdvPqE8GI_LM"},"source":["#11.6.3.2. 标量函数\n","lambdas = [0.1, 1, 10, 19]\n","eta = 0.1\n","d2l.set_figsize((6, 4))\n","for lam in lambdas:\n","    t = torch.arange(20).detach().numpy()\n","    d2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')\n","d2l.plt.xlabel('time')\n","d2l.plt.legend();"],"execution_count":null,"outputs":[]}]}