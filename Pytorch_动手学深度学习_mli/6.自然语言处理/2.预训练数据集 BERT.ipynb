{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.预训练数据集 BERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOoDZ58irjuEU0rYgMZvJ9F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"13Hx--s5DWPH"},"source":["#与 14.3节 中用于预训 word2vec 的 PTB 数据集相比，WikiText-2 (i) 保留了原始标点符号，\n","#使其适合于下一句预测；(ii) 保留原始大小写和数字；(iii) 大两倍以上。\n","!pip install d2l\n","import os\n","import random\n","import torch\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOFmxqDxDmh1"},"source":["#在 Wikitext-2 数据集中，每行代表一个段落，其中任何标点符号和前面的词元之间插入空格。\n","#保留至少有两句话的段落。为了分割句子，为了简单起见，我们只使用句点作为分隔符。\n","#我们将在本节末尾的练习中讨论更复杂的句子分割技术。\n","d2l.DATA_HUB['wikitext-2'] = (\n","    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n","    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n","\n","def _read_wiki(data_dir):\n","    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n","    with open(file_name, 'r') as f:\n","        lines = f.readlines()\n","    # Uppercase letters are converted to lowercase ones\n","    paragraphs = [line.strip().lower().split(' . ')\n","                  for line in lines if len(line.split(' . ')) >= 2]\n","    random.shuffle(paragraphs)\n","    return paragraphs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xx8fb7CrDmnb"},"source":["#生成下一句预测任务\n","#根据 subsec_nsp 的描述，_get_next_sentence 函数生成了二进制分类任务的训练示例。\n","def _get_next_sentence(sentence, next_sentence, paragraphs):\n","    if random.random() < 0.5:\n","        is_next = True\n","    else:\n","        # `paragraphs` is a list of lists of lists\n","        next_sentence = random.choice(random.choice(paragraphs))\n","        is_next = False\n","    return sentence, next_sentence, is_next"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rH5e4XDfDmrK"},"source":["#以下函数通过调用 _get_next_sentence 函数生成从输入 paragraph 进行下一句预测的训练示例。\n","#这里 paragraph 是一个句子列表，其中每句都是一个词元列表。参数 max_len 指定了预训期间 BERT 输入序列的最大长度。\n","def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n","    nsp_data_from_paragraph = []\n","    for i in range(len(paragraph) - 1):\n","        tokens_a, tokens_b, is_next = _get_next_sentence(\n","            paragraph[i], paragraph[i + 1], paragraphs)\n","        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n","        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n","            continue\n","        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n","        nsp_data_from_paragraph.append((tokens, segments, is_next))\n","    return nsp_data_from_paragraph"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vl43JQ98DmvL"},"source":["#生成蒙版语言建模任务\n","def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n","                        vocab):\n","    # Make a new copy of tokens for the input of a masked language model,\n","    # where the input may contain replaced '<mask>' or random tokens\n","    mlm_input_tokens = [token for token in tokens]\n","    pred_positions_and_labels = []\n","    # Shuffle for getting 15% random tokens for prediction in the masked\n","    # language modeling task\n","    random.shuffle(candidate_pred_positions)\n","    for mlm_pred_position in candidate_pred_positions:\n","        if len(pred_positions_and_labels) >= num_mlm_preds:\n","            break\n","        masked_token = None\n","        # 80% of the time: replace the word with the '<mask>' token\n","        if random.random() < 0.8:\n","            masked_token = '<mask>'\n","        else:\n","            # 10% of the time: keep the word unchanged\n","            if random.random() < 0.5:\n","                masked_token = tokens[mlm_pred_position]\n","            # 10% of the time: replace the word with a random word\n","            else:\n","                masked_token = random.randint(0, len(vocab) - 1)\n","        mlm_input_tokens[mlm_pred_position] = masked_token\n","        pred_positions_and_labels.append(\n","            (mlm_pred_position, tokens[mlm_pred_position]))\n","    return mlm_input_tokens, pred_positions_and_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"349ZxDRSDmzb"},"source":["#通过调用上述 _replace_mlm_tokens 函数，以下函数将 BERT 输入序列 (tokens) 作为输入序列 (tokens) \n","#并返回输入词元的索引（在可能的词元替换后，如 subsec_mlm 所述）、发生预测的词元指数以及词元这些指数的索引预测。\n","def _get_mlm_data_from_tokens(tokens, vocab):\n","    candidate_pred_positions = []\n","    # `tokens` is a list of strings\n","    for i, token in enumerate(tokens):\n","        # Special tokens are not predicted in the masked language modeling\n","        # task\n","        if token in ['<cls>', '<sep>']:\n","            continue\n","        candidate_pred_positions.append(i)\n","    # 15% of random tokens are predicted in the masked language modeling task\n","    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n","    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n","        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n","    pred_positions_and_labels = sorted(pred_positions_and_labels,\n","                                       key=lambda x: x[0])\n","    pred_positions = [v[0] for v in pred_positions_and_labels]\n","    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n","    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2XCSiYd-El9U"},"source":["#将文本转换为训练前数据集\n","def _pad_bert_inputs(examples, max_len, vocab):\n","    max_num_mlm_preds = round(max_len * 0.15)\n","    all_token_ids, all_segments, valid_lens,  = [], [], []\n","    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n","    nsp_labels = []\n","    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n","         is_next) in examples:\n","        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n","            max_len - len(token_ids)), dtype=torch.long))\n","        all_segments.append(torch.tensor(segments + [0] * (\n","            max_len - len(segments)), dtype=torch.long))\n","        # `valid_lens` excludes count of '<pad>' tokens\n","        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n","        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n","            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n","        # Predictions of padded tokens will be filtered out in the loss via\n","        # multiplication of 0 weights\n","        all_mlm_weights.append(\n","            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n","                max_num_mlm_preds - len(pred_positions)),\n","                dtype=torch.float32))\n","        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n","            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n","        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n","    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n","            all_mlm_weights, all_mlm_labels, nsp_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YDcY0ZVEp97"},"source":["class _WikiTextDataset(torch.utils.data.Dataset):\n","    def __init__(self, paragraphs, max_len):\n","        # Input `paragraphs[i]` is a list of sentence strings representing a\n","        # paragraph; while output `paragraphs[i]` is a list of sentences\n","        # representing a paragraph, where each sentence is a list of tokens\n","        paragraphs = [d2l.tokenize(\n","            paragraph, token='word') for paragraph in paragraphs]\n","        sentences = [sentence for paragraph in paragraphs\n","                     for sentence in paragraph]\n","        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n","            '<pad>', '<mask>', '<cls>', '<sep>'])\n","        # Get data for the next sentence prediction task\n","        examples = []\n","        for paragraph in paragraphs:\n","            examples.extend(_get_nsp_data_from_paragraph(\n","                paragraph, paragraphs, self.vocab, max_len))\n","        # Get data for the masked language model task\n","        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n","                      + (segments, is_next))\n","                     for tokens, segments, is_next in examples]\n","        # Pad inputs\n","        (self.all_token_ids, self.all_segments, self.valid_lens,\n","         self.all_pred_positions, self.all_mlm_weights,\n","         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n","            examples, max_len, self.vocab)\n","\n","    def __getitem__(self, idx):\n","        return (self.all_token_ids[idx], self.all_segments[idx],\n","                self.valid_lens[idx], self.all_pred_positions[idx],\n","                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n","                self.nsp_labels[idx])\n","\n","    def __len__(self):\n","        return len(self.all_token_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25BCAQeJEsnT"},"source":["def load_data_wiki(batch_size, max_len):\n","    \"\"\"Load the WikiText-2 dataset.\"\"\"\n","    num_workers = d2l.get_dataloader_workers()\n","    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n","    paragraphs = _read_wiki(data_dir)\n","    train_set = _WikiTextDataset(paragraphs, max_len)\n","    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n","                                        shuffle=True, num_workers=num_workers)\n","    return train_iter, train_set.vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0bGBlsDEvMz"},"source":["batch_size, max_len = 512, 64\n","train_iter, vocab = load_data_wiki(batch_size, max_len)\n","\n","for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n","     mlm_Y, nsp_y) in train_iter:\n","    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n","          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n","          nsp_y.shape)\n","    break"],"execution_count":null,"outputs":[]}]}