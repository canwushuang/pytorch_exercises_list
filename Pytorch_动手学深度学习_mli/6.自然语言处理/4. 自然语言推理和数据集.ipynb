{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.4. 自然语言推理和数据集.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO6vnWikY6bbg3lz3flaZ1N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ej_WOViEFeMR"},"source":["#1.4.2. 斯坦福大学自然语言推理 (SNLI) 数据集¶\n","#斯坦福大学自然语言推理 (SNLI) 语料库是超过 50 万个标记为英语句子对 [Bowman et al., 2015] 的集合。\n","#我们将提取的 SNLI 数据集下载并存储在路径 ../data/snli_1.0 中。\n","!pip install d2l\n","import os\n","import re\n","import torch\n","from torch import nn\n","from d2l import torch as d2l\n","\n","d2l.DATA_HUB['SNLI'] = (\n","    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n","    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n","\n","data_dir = d2l.download_extract('SNLI')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wF3mP_nGI8H"},"source":["#1.4.2.1. 阅读数据集\n","#原始 SNLI 数据集包含的信息比我们实验中真正需要的信息丰富得多。\n","#因此，我们定义了一个函数 read_snli 来仅提取部分数据集，然后返回前提、假设及其标签的列表。\n","def read_snli(data_dir, is_train):\n","    \"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\n","    def extract_text(s):\n","        # Remove information that will not be used by us\n","        s = re.sub('\\\\(', '', s)\n","        s = re.sub('\\\\)', '', s)\n","        # Substitute two or more consecutive whitespace with space\n","        s = re.sub('\\\\s{2,}', ' ', s)\n","        return s.strip()\n","    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n","    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n","                             if is_train else 'snli_1.0_test.txt')\n","    with open(file_name, 'r') as f:\n","        rows = [row.split('\\t') for row in f.readlines()[1:]]\n","    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n","    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n","    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n","    return premises, hypotheses, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fLJ5WpXoGV35"},"source":["#现在让我们打印前 3 对前提和假设，以及它们的标签（“0”、“1” 和 “2” 分别对应于 “包容”、“矛盾” 和 “中性”）。\n","train_data = read_snli(data_dir, is_train=True)\n","for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n","    print('premise:', x0)\n","    print('hypothesis:', x1)\n","    print('label:', y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-B0NiHHGGZSJ","executionInfo":{"status":"ok","timestamp":1628566051226,"user_tz":-480,"elapsed":9,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["#该训练套装有大约 550,000 对，测试套装有约 10000 对。\n","#以下表明，在培训套装和测试套装中，“包容”、“矛盾” 和 “中性” 三个标签都是平衡的。\n","test_data = read_snli(data_dir, is_train=False)\n","for data in [train_data, test_data]:\n","    print([[row for row in data[2]].count(i) for i in range(3)])"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvNDvaBsGaf5"},"source":["#1.4.2.2. 定义用于加载数据集的类\n","class SNLIDataset(torch.utils.data.Dataset):\n","    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n","    def __init__(self, dataset, num_steps, vocab=None):\n","        self.num_steps = num_steps\n","        all_premise_tokens = d2l.tokenize(dataset[0])\n","        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n","        if vocab is None:\n","            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n","                                   min_freq=5, reserved_tokens=['<pad>'])\n","        else:\n","            self.vocab = vocab\n","        self.premises = self._pad(all_premise_tokens)\n","        self.hypotheses = self._pad(all_hypothesis_tokens)\n","        self.labels = torch.tensor(dataset[2])\n","        print('read ' + str(len(self.premises)) + ' examples')\n","\n","    def _pad(self, lines):\n","        return torch.tensor([d2l.truncate_pad(\n","            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n","                         for line in lines])\n","\n","    def __getitem__(self, idx):\n","        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n","\n","    def __len__(self):\n","        return len(self.premises)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"achowXvyGos5"},"source":["#1.4.2.3. 把所有东西放在一起¶\n","#现在我们可以调用 read_snli 函数和 SNLIDataset 类来下载 SNLI 数据集，\n","#并返回 DataLoader 实例用于训练和测试集合，以及训练集的词汇表。值得注意的是，我们必须使用从训练集中构建的词汇和测试集合的词汇。因此，在训练集中训练的模型将不知道测试集中的任何新令牌。\n","def load_data_snli(batch_size, num_steps=50):\n","    \"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\n","    num_workers = d2l.get_dataloader_workers()\n","    data_dir = d2l.download_extract('SNLI')\n","    train_data = read_snli(data_dir, True)\n","    test_data = read_snli(data_dir, False)\n","    train_set = SNLIDataset(train_data, num_steps)\n","    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n","    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n","                                             shuffle=True,\n","                                             num_workers=num_workers)\n","    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n","                                            shuffle=False,\n","                                            num_workers=num_workers)\n","    return train_iter, test_iter, train_set.vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XiTezaJ-GwkK"},"source":["#在这里，我们将批处理大小设置为 128，序列长度为 50，然后调用 load_data_snli 函数来获取数据迭代器和词汇。\n","#然后我们打印词汇量大小。\n","train_iter, test_iter, vocab = load_data_snli(128, 50)\n","len(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJkP6EY7G0z5"},"source":["#现在我们打印第一个迷你手表的形状。与情绪分析相反，我们有两个输入 X[0] 和 X[1]，代表对前提和假设。\n","for X, Y in train_iter:\n","    print(X[0].shape)\n","    print(X[1].shape)\n","    print(Y.shape)\n","    break"],"execution_count":null,"outputs":[]}]}