{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5. 自然语言推理：微调 BERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOoXjh2ZCt1yT/GuWTRuJfa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"o9x3rH_LHFAL"},"source":["#在本节中，我们将下载预训练的 BERT 小版本，然后对其进行微调，以便在 SNLI 数据集上推断自然语言。\n","!pip install d2l\n","import json\n","import multiprocessing\n","import os\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UtJEZKbHJr2"},"source":["#1.7.1. 加载预训练的 BERT\n","d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n","                             '225d66f04cae318b841a13d32af3acc165f253ac')\n","d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n","                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQ258J6gHVOR"},"source":["#任何一个预训练的 BERT 模型都包含一个定义词汇集的 “vocab.json” 文件和预训练参数的 “预训练的 .params” 文件。\n","#我们实现了以下 load_pretrained_model 函数来加载预训练的 BERT 参数。\n","def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n","                          num_heads, num_layers, dropout, max_len, devices):\n","    data_dir = d2l.download_extract(pretrained_model)\n","    # Define an empty vocabulary to load the predefined vocabulary\n","    vocab = d2l.Vocab()\n","    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n","    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n","        vocab.idx_to_token)}\n","    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n","                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n","                         num_heads=4, num_layers=2, dropout=0.2,\n","                         max_len=max_len, key_size=256, query_size=256,\n","                         value_size=256, hid_in_features=256,\n","                         mlm_in_features=256, nsp_in_features=256)\n","    # Load pretrained BERT parameters\n","    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n","    return bert, vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6b0Gb1cHbHh"},"source":["#为了方便在大多数机器上进行演示，我们将在本节中加载和微调预训练的 BERT 的小版本（“bert.mall”）。\n","#在练习中，我们将展示如何微调更大的 “bert.base”，以显著提高测试准确性。\n","devices = d2l.try_all_gpus()\n","bert, vocab = load_pretrained_model(\n","    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n","    num_layers=2, dropout=0.1, max_len=512, devices=devices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PN3s0JzHeBU"},"source":["#1.7.2. 微调 BERT 的数据集\n","class SNLIBERTDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, max_len, vocab=None):\n","        all_premise_hypothesis_tokens = [[\n","            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n","            *[d2l.tokenize([s.lower() for s in sentences])\n","              for sentences in dataset[:2]])]\n","\n","        self.labels = torch.tensor(dataset[2])\n","        self.vocab = vocab\n","        self.max_len = max_len\n","        (self.all_token_ids, self.all_segments,\n","         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n","        print('read ' + str(len(self.all_token_ids)) + ' examples')\n","\n","    def _preprocess(self, all_premise_hypothesis_tokens):\n","        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n","        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n","        all_token_ids = [\n","            token_ids for token_ids, segments, valid_len in out]\n","        all_segments = [segments for token_ids, segments, valid_len in out]\n","        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n","        return (torch.tensor(all_token_ids, dtype=torch.long),\n","                torch.tensor(all_segments, dtype=torch.long),\n","                torch.tensor(valid_lens))\n","\n","    def _mp_worker(self, premise_hypothesis_tokens):\n","        p_tokens, h_tokens = premise_hypothesis_tokens\n","        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n","        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n","        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n","                             * (self.max_len - len(tokens))\n","        segments = segments + [0] * (self.max_len - len(segments))\n","        valid_len = len(tokens)\n","        return token_ids, segments, valid_len\n","\n","    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n","        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n","        # input\n","        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n","            if len(p_tokens) > len(h_tokens):\n","                p_tokens.pop()\n","            else:\n","                h_tokens.pop()\n","\n","    def __getitem__(self, idx):\n","        return (self.all_token_ids[idx], self.all_segments[idx],\n","                self.valid_lens[idx]), self.labels[idx]\n","\n","    def __len__(self):\n","        return len(self.all_token_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1SwKQ2cHhh1"},"source":["#下载 SNLI 数据集之后，我们通过实例化 SNLIBERTDataset 类来生成训练和测试示例。\n","#在自然语言推理的训练和测试期间，这些例子将通过迷你小册子阅读。\n","# Reduce `batch_size` if there is an out of memory error. In the original BERT\n","# model, `max_len` = 512\n","batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n","data_dir = d2l.download_extract('SNLI')\n","train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n","test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n","train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n","                                   num_workers=num_workers)\n","test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n","                                  num_workers=num_workers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ct73khlMHk10"},"source":["#1.7.3. 微调 BERT\n","#正如 fig_bert-two-seqs 所示，对自然语言推断进行微调 BERT 只需要额外的 MLP，\n","#由两个完全连接的图层组成（见下面的 self.hidden 和 self.output）。该 MLP 将对前提和假设的信息进行编码的特殊 “” 令牌的 BERT 表示转换为自然语言推理的三个输出：内容、矛盾和中性。\n","class BERTClassifier(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTClassifier, self).__init__()\n","        self.encoder = bert.encoder\n","        self.hidden = bert.hidden\n","        self.output = nn.Linear(256, 3)\n","\n","    def forward(self, inputs):\n","        tokens_X, segments_X, valid_lens_x = inputs\n","        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n","        return self.output(self.hidden(encoded_X[:, 0, :]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZAlhtIkHnZz"},"source":["net = BERTClassifier(bert)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pI4WuIHIHssM"},"source":["lr, num_epochs = 1e-4, 5\n","trainer = torch.optim.Adam(net.parameters(), lr=lr)\n","loss = nn.CrossEntropyLoss(reduction='none')\n","d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"],"execution_count":null,"outputs":[]}]}