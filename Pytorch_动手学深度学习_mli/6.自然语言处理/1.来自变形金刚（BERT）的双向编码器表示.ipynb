{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.来自变形金刚（BERT）的双向编码器表示.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOvQjqnREeCdYY+1mtkuVa6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"W0nUeVPICRvu"},"source":["#在本章的其余部分，我们将深入研究 BERT 的预培训。\n","#当 1节 中解释自然语言处理应用程序时，我们将说明对下游应用程序的 BERT 的微调。\n","!pip install d2l\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5smGtxnCeyf"},"source":["#输入表示法\n","#以下 get_tokens_and_segments 以一句或两句话作为输入，然后返回 BERT 输入序列的词元及其对应的段 ID。\n","def get_tokens_and_segments(tokens_a, tokens_b=None):\n","    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n","    tokens = ['<cls>'] + tokens_a + ['<sep>']\n","    # 0 and 1 are marking segment A and B, respectively\n","    segments = [0] * (len(tokens_a) + 2)\n","    if tokens_b is not None:\n","        tokens += tokens_b + ['<sep>']\n","        segments += [1] * (len(tokens_b) + 1)\n","    return tokens, segments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EyLduE6OCkC1"},"source":["#以下 BERTEncoder 类与 10.7节 中实施的 TransformerEncoder 类类似。\n","#与 TransformerEncoder 不同，BERTEncoder 使用细分嵌入和可学习的位置嵌入。\n","class BERTEncoder(nn.Module):\n","    \"\"\"BERT encoder.\"\"\"\n","    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n","                 ffn_num_hiddens, num_heads, num_layers, dropout,\n","                 max_len=1000, key_size=768, query_size=768, value_size=768,\n","                 **kwargs):\n","        super(BERTEncoder, self).__init__(**kwargs)\n","        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n","        self.segment_embedding = nn.Embedding(2, num_hiddens)\n","        self.blks = nn.Sequential()\n","        for i in range(num_layers):\n","            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n","                key_size, query_size, value_size, num_hiddens, norm_shape,\n","                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n","        # In BERT, positional embeddings are learnable, thus we create a\n","        # parameter of positional embeddings that are long enough\n","        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n","                                                      num_hiddens))\n","\n","    def forward(self, tokens, segments, valid_lens):\n","        # Shape of `X` remains unchanged in the following code snippet:\n","        # (batch size, max sequence length, `num_hiddens`)\n","        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n","        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n","        for blk in self.blks:\n","            X = blk(X, valid_lens)\n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7twem8EyCpAl"},"source":["#假设词汇量大小是 10000。为了演示 BERTEncoder 的前向推理，让我们创建它的实例并初始化其参数。\n","vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n","norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n","encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n","                      ffn_num_hiddens, num_heads, num_layers, dropout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"id":"j3BEytS3Cq49","executionInfo":{"status":"error","timestamp":1628565090738,"user_tz":-480,"elapsed":7,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"e169583b-0839-4eea-8f07-c6fd3d1911f2"},"source":["#我们将 tokens 定义为 2 个长度为 8 的 BERT 输入序列，其中每个词元都是词汇的索引。\n","#输入 BERTEncoder 的 BERTEncoder 和输入 tokens 返回编码结果，其中每个词元由超参数 num_hiddens 预定义的向量表示，\n","#其长度由超参数 num_hiddens 预定义。此超参数通常称为变压器编码器的 * 隐藏大小 *（隐藏单位数）。\n","tokens = torch.randint(0, vocab_size, (2, 8))\n","segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n","encoded_X = encoder(tokens, segments, None)\n","encoded_X.shape"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bd5248139383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#我们将 tokens 定义为 2 个长度为 8 的 BERT 输入序列，其中每个词元都是词汇的索引。输入 BERTEncoder 的 BERTEncoder 和输入 tokens 返回编码结果，其中每个词元由超参数 num_hiddens 预定义的向量表示，其长度由超参数 num_hiddens 预定义。此超参数通常称为变压器编码器的 * 隐藏大小 *（隐藏单位数）。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoded_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoded_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_fKwVfkPCwX9"},"source":["#蒙面语言建模\n","#我们实施了以下 MaskLM 课程来预测 BERT 预训的蒙面语言模型任务中的蒙面词元。\n","#该预测使用一个隐藏层 MLP（self.mlp）。\n","#在前向推断中，它需要两个输入：BERTEncoder 的编码结果和用于预测的代币位置。输出是这些仓位的预测结果。\n","class MaskLM(nn.Module):\n","    \"\"\"The masked language model task of BERT.\"\"\"\n","    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n","        super(MaskLM, self).__init__(**kwargs)\n","        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n","                                 nn.ReLU(),\n","                                 nn.LayerNorm(num_hiddens),\n","                                 nn.Linear(num_hiddens, vocab_size))\n","\n","    def forward(self, X, pred_positions):\n","        num_pred_positions = pred_positions.shape[1]\n","        pred_positions = pred_positions.reshape(-1)\n","        batch_size = X.shape[0]\n","        batch_idx = torch.arange(0, batch_size)\n","        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n","        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n","        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n","        masked_X = X[batch_idx, pred_positions]\n","        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n","        mlm_Y_hat = self.mlp(masked_X)\n","        return mlm_Y_hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EG6zJG8C9V9"},"source":["mlm = MaskLM(vocab_size, num_hiddens)\n","mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n","mlm_Y_hat = mlm(encoded_X, mlm_positions)\n","mlm_Y_hat.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRWe3yEFC9t8"},"source":["#通过掩码下的预测词元 mlm_Y_hat 的地面真相标签 mlm_Y，我们可以计算 BERT 预训练中蒙面语言模型任务的交叉熵损失。\n","mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n","loss = nn.CrossEntropyLoss(reduction='none')\n","mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n","mlm_l.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqx8bqhDDBpt"},"source":["#下一句预测\n","class NextSentencePred(nn.Module):\n","    \"\"\"The next sentence prediction task of BERT.\"\"\"\n","    def __init__(self, num_inputs, **kwargs):\n","        super(NextSentencePred, self).__init__(**kwargs)\n","        self.output = nn.Linear(num_inputs, 2)\n","\n","    def forward(self, X):\n","        # `X` shape: (batch size, `num_hiddens`)\n","        return self.output(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUF9VvsEDFXe"},"source":["#我们可以看到，NextSentencePred 实例的前向推断返回每个 BERT 输入序列的二进制预测。\n","# PyTorch by default won't flatten the tensor as seen in mxnet where, if\n","# flatten=True, all but the first axis of input data are collapsed together\n","encoded_X = torch.flatten(encoded_X, start_dim=1)\n","# input_shape for NSP: (batch size, `num_hiddens`)\n","nsp = NextSentencePred(encoded_X.shape[-1])\n","nsp_Y_hat = nsp(encoded_X)\n","nsp_Y_hat.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYwR0te_DIFF"},"source":["#还可以计算两个二进制分类的交叉熵损失。\n","nsp_y = torch.tensor([0, 1])\n","nsp_l = loss(nsp_Y_hat, nsp_y)\n","nsp_l.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdyDJi5bDMON"},"source":["#把所有东西放在一起\n","#在预训练 BERT 时，最终损失函数是掩码语言建模的损失函数和下一句预测的线性组合。\n","#现在我们可以通过实例化三个类 BERTEncoder、MaskLM 和 NextSentencePred 来定义 BERTModel 类。\n","#前向推理返回编码的 BERT 表示 encoded_X、对蒙面语言建模 mlm_Y_hat 的预测以及下一句预测 nsp_Y_hat。\n","class BERTModel(nn.Module):\n","    \"\"\"The BERT model.\"\"\"\n","    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n","                 ffn_num_hiddens, num_heads, num_layers, dropout,\n","                 max_len=1000, key_size=768, query_size=768, value_size=768,\n","                 hid_in_features=768, mlm_in_features=768,\n","                 nsp_in_features=768):\n","        super(BERTModel, self).__init__()\n","        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n","                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n","                    dropout, max_len=max_len, key_size=key_size,\n","                    query_size=query_size, value_size=value_size)\n","        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n","                                    nn.Tanh())\n","        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n","        self.nsp = NextSentencePred(nsp_in_features)\n","\n","    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n","        encoded_X = self.encoder(tokens, segments, valid_lens)\n","        if pred_positions is not None:\n","            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n","        else:\n","            mlm_Y_hat = None\n","        # The hidden layer of the MLP classifier for next sentence prediction.\n","        # 0 is the index of the '<cls>' token\n","        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n","        return encoded_X, mlm_Y_hat, nsp_Y_hat"],"execution_count":null,"outputs":[]}]}