{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.1. 层和块.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMtLt2P+KsKNqPiO0SoLyQY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQnpgDA4UGgg","executionInfo":{"status":"ok","timestamp":1619310099147,"user_tz":-480,"elapsed":5513,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"187d0df4-14c1-48c3-b00e-d42df56e278e"},"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","\n","net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","\n","X = torch.rand(2, 20)\n","net(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0765, -0.0565,  0.0328,  0.1257, -0.0694,  0.0054,  0.1751, -0.0974,\n","          0.0056,  0.1043],\n","        [ 0.0237, -0.1911,  0.1564, -0.0314, -0.0103, -0.0017,  0.0606, -0.0023,\n","          0.1180, -0.0565]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyWHp6lZUSRo","executionInfo":{"status":"ok","timestamp":1619310099148,"user_tz":-480,"elapsed":5510,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"1e79b8ce-99ec-44b3-b915-d826e01a1e1f"},"source":["#5.1.1. 自定义块\n","#在下面的代码片段中，我们从零开始编写一个块。它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。\n","#注意，下面的MLP类继承了表示块的类。我们的实现将严重依赖父类，只需要提供我们自己的构造函数（Python中的__init__函数）和正向传播函数。\n","class MLP(nn.Module):\n","    # 用模型参数声明层。这里，我们声明两个全连接的层\n","    def __init__(self):\n","        # 调用`MLP`的父类`Block`的构造函数来执行必要的初始化。\n","        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数`params`（稍后将介绍）\n","        super().__init__()\n","        self.hidden = nn.Linear(20, 256)  # 隐藏层\n","        self.out = nn.Linear(256, 10)  # 输出层\n","\n","    # 定义模型的正向传播，即如何根据输入`X`返回所需的模型输出\n","    def forward(self, X):\n","        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n","        return self.out(F.relu(self.hidden(X)))\n","\n","net = MLP()\n","net(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1734,  0.1954,  0.0547,  0.0695,  0.1267, -0.1656, -0.0765, -0.0950,\n","          0.1621, -0.1284],\n","        [-0.1081,  0.1979,  0.1222,  0.0552,  0.0938, -0.1893, -0.0279, -0.0799,\n","          0.2117, -0.1701]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSbC09LCUzYY","executionInfo":{"status":"ok","timestamp":1619310099148,"user_tz":-480,"elapsed":5508,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"3f44b19d-e7ae-49b5-dbbf-0b811383dfb8"},"source":["#5.1.2. 顺序块\n","#现在我们可以更仔细地看看Sequential类是如何工作的。回想一下Sequential的设计是为了把其他模块串起来。\n","#为了构建我们自己的简化的MySequential，我们只需要定义两个关键函数： \n","#1. 一种将块逐个追加到列表中的函数。 \n","#2. 一种正向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n","#下面的MySequential类提供了与默认Sequential类相同的功能。\n","class MySequential(nn.Module):\n","    def __init__(self, *args):\n","        super().__init__()\n","        for block in args:\n","            # 这里，`block`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量\n","            # `_children` 中。`block`的类型是OrderedDict。\n","            self._modules[block] = block\n","\n","    def forward(self, X):\n","        # OrderedDict保证了按照成员添加的顺序遍历它们\n","        for block in self._modules.values():\n","            X = block(X)\n","        return X\n","\n","net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n","net(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.3967, -0.1215, -0.0100,  0.0486, -0.0293,  0.1209, -0.1003,  0.1667,\n","         -0.1811,  0.1829],\n","        [-0.3784, -0.0129,  0.1796, -0.0364,  0.0321,  0.1733, -0.0272,  0.2743,\n","         -0.1667,  0.1450]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukc_8avWVNJy","executionInfo":{"status":"ok","timestamp":1619310099149,"user_tz":-480,"elapsed":5506,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"a3d8159a-119b-4127-beb4-83d7db653636"},"source":["#5.1.3. 在正向传播函数中执行代码\n","#到目前为止，我们网络中的所有操作都对网络的激活值及网络的参数起作用。\n","#然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项。\n","#我们称之为常数参数（constant parameters）。例如，我们需要一个计算函数 f(x,w)=c⋅w⊤x 的层，\n","#其中 x 是输入， w 是我们的参数， c 是某个在优化过程中没有更新的指定常量。因此我们实现了一个FixedHiddenMLP类，如下所示。\n","class FixedHiddenMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # 不计算梯度的随机权重参数。因此其在训练期间保持不变。\n","        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n","        self.linear = nn.Linear(20, 20)\n","\n","    def forward(self, X):\n","        X = self.linear(X)\n","        # 使用创建的常量参数以及`relu`和`dot`函数。\n","        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n","        # 复用全连接层。这相当于两个全连接层共享参数。\n","        X = self.linear(X)\n","        # 控制流\n","        while X.abs().sum() > 1:\n","            X /= 2\n","        return X.sum()\n","\n","net = FixedHiddenMLP()\n","net(X)        "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.0869, grad_fn=<SumBackward0>)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QExFCpHCVbky","executionInfo":{"status":"ok","timestamp":1619310099150,"user_tz":-480,"elapsed":5506,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"226bbad1-3bea-492f-a9f0-04fe3fb72169"},"source":["#我们可以混合搭配各种组合块的方法。在下面的例子中，我们以一些想到的的方法嵌套块。\n","class NestMLP(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n","                      nn.Linear(64, 32), nn.ReLU())\n","        self.linear = nn.Linear(32, 16)\n","\n","    def forward(self, X):\n","        return self.linear(self.net(X))\n","\n","chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n","chimera(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.1855, grad_fn=<SumBackward0>)"]},"metadata":{"tags":[]},"execution_count":5}]}]}