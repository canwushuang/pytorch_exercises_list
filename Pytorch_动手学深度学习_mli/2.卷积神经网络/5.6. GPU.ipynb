{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.6. GPU.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPaAenRAJ1LU12MhlFraAuc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-RNyX46h8uD","executionInfo":{"status":"ok","timestamp":1619312951131,"user_tz":-480,"elapsed":815,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"1caf1d66-fa3e-4df3-9dbe-72a2b66ad1d4"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun Apr 25 01:09:11 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRiyOaSyjYGA","executionInfo":{"status":"ok","timestamp":1619312955058,"user_tz":-480,"elapsed":4729,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"4023e784-efca-4d7f-91a4-1fd907d91b1e"},"source":["#5.6.1. 计算设备¶\n","#我们可以指定用于存储和计算的设备，如CPU和GPU。默认情况下，张量是在内存中创建的，然后使用CPU计算它。\n","#在PyTorch中，CPU和GPU可以用torch.device('cpu')和torch.cuda.device('cuda')表示。\n","#应该注意的是，cpu设备意味着所有物理CPU和内存。这意味着PyTorch的计算将尝试使用所有CPU核心。\n","#然而，gpu设备只代表一个卡和相应的显存。如果有多个GPU，我们使用torch.cuda.device(f'cuda:{i}')来表示第 i 块GPU（ i 从0开始）。另外，gpu:0和gpu是等价的。\n","import torch\n","from torch import nn\n","\n","torch.device('cpu'), torch.cuda.device('cuda'), torch.cuda.device('cuda:1')"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cpu'),\n"," <torch.cuda.device at 0x7f761b9c6590>,\n"," <torch.cuda.device at 0x7f7617793950>)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kjki9AUGjlqN","executionInfo":{"status":"ok","timestamp":1619312955059,"user_tz":-480,"elapsed":4724,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"f9cc6541-943c-48a4-ac70-96d7437e412a"},"source":["#我们可以查询可用gpu的数量。\n","torch.cuda.device_count()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LHi-vHDjrrm","executionInfo":{"status":"ok","timestamp":1619312955060,"user_tz":-480,"elapsed":4718,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"2536499b-5e51-4949-bf1e-4dae99ba06bf"},"source":["#现在我们定义了两个方便的函数，这两个函数允许我们在请求的GPU不存在的情况下运行代码。\n","def try_gpu(i=0):  #@save\n","    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()。\"\"\"\n","    if torch.cuda.device_count() >= i + 1:\n","        return torch.device(f'cuda:{i}')\n","    return torch.device('cpu')\n","\n","def try_all_gpus():  #@save\n","    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。\"\"\"\n","    devices = [\n","        torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n","    return devices if devices else [torch.device('cpu')]\n","\n","try_gpu(), try_gpu(10), try_all_gpus()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(device(type='cuda', index=0),\n"," device(type='cpu'),\n"," [device(type='cuda', index=0)])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbNXXLPgjwPj","executionInfo":{"status":"ok","timestamp":1619312955060,"user_tz":-480,"elapsed":4712,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"5ec8ae45-f570-4cc7-c0f2-ea6193d85b4e"},"source":["#5.6.2. 张量与gpu¶\n","#默认情况下，张量是在CPU上创建的。我们可以查询张量所在的设备。\n","x = torch.tensor([1, 2, 3])\n","x.device"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOuRu5qYj4Ru","executionInfo":{"status":"ok","timestamp":1619312965671,"user_tz":-480,"elapsed":15319,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"4b71c77b-c54f-4acb-b39f-007ba5ebc274"},"source":["#5.6.2.1. 存储在GPU上\n","#有几种方法可以在GPU上存储张量。例如，我们可以在创建张量时指定存储设备。接下来，\n","#我们在第一个gpu上创建张量变量X。在GPU上创建的张量只消耗这个GPU的显存。\n","#我们可以使用nvidia-smi命令查看显存使用情况。一般来说，我们需要确保不创建超过GPU显存限制的数据。\n","X = torch.ones(2, 3, device=try_gpu())\n","X"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyOBdw3_kAvC","executionInfo":{"status":"ok","timestamp":1619312965672,"user_tz":-480,"elapsed":15315,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"30628696-4d81-44cd-b3c9-eaf997d326f5"},"source":["#假设你至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量。\n","Y = torch.rand(2, 3, device=try_gpu(1))\n","Y"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1091, 0.8960, 0.0766],\n","        [0.7553, 0.9326, 0.1911]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOZseKqmkGHd","executionInfo":{"status":"ok","timestamp":1619312982532,"user_tz":-480,"elapsed":657,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"737cdfad-2b2e-48af-ff38-8471e1a982f6"},"source":["#5.6.2.2. 复制\n","Z = X.cuda()\n","print(X)\n","print(Z)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"Eg7bEoylkJ-e","executionInfo":{"status":"error","timestamp":1619312985826,"user_tz":-480,"elapsed":781,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"79b7fb38-d8f6-42de-8304-a3e871354761"},"source":["#现在数据在同一个GPU上（Z和Y都在），我们可以将它们相加。\n","Y + Z"],"execution_count":10,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-adb796b87e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#现在数据在同一个GPU上（Z和Y都在），我们可以将它们相加。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"]}]},{"cell_type":"code","metadata":{"id":"_a7lUiMPkLep","executionInfo":{"status":"aborted","timestamp":1619312966091,"user_tz":-480,"elapsed":15718,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["#假设变量Z已经存在于第二个GPU上。如果我们还是调用Z.cuda(1)怎么办？它将返回Z，而不会复制并分配新内存。\n","Z.cuda(1) is Z"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4s-ZlJ7vkSMh","executionInfo":{"status":"aborted","timestamp":1619312966092,"user_tz":-480,"elapsed":15716,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["#5.6.3. 神经网络与GPU¶\n","#类似地，神经网络模型可以指定设备。下面的代码将模型参数放在GPU上。\n","net = nn.Sequential(nn.Linear(3, 1))\n","net = net.to(device=try_gpu())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZayyM5eYkXFr","executionInfo":{"status":"aborted","timestamp":1619312966093,"user_tz":-480,"elapsed":15712,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["#当输入为GPU上的张量时，模型将在同一GPU上计算结果。\n","net(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWJ2YvNfkaPX","executionInfo":{"status":"aborted","timestamp":1619312966094,"user_tz":-480,"elapsed":15709,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["#让我们确认模型参数存储在同一个GPU上。\n","net[0].weight.data.device"],"execution_count":null,"outputs":[]}]}