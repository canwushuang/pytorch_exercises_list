{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5.2. 参数管理.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPx12eEcKZ9cU8bUW9SAVJ2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCdxyUdKZ-BF","executionInfo":{"status":"ok","timestamp":1619310638759,"user_tz":-480,"elapsed":3876,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"e3ee0b58-f056-4bad-9375-55de5ef63de5"},"source":["import torch\n","from torch import nn\n","\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n","X = torch.rand(size=(2, 4))\n","net(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1129],\n","        [0.1855]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFzRYi4PaD6e","executionInfo":{"status":"ok","timestamp":1619310638760,"user_tz":-480,"elapsed":3873,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"6d0b2afb-71e3-4589-84a5-f6a0d9811fd4"},"source":["#5.2.1. 参数访问\n","#我们从已有模型中访问参数。当通过Sequential类定义模型时，我们可以通过索引来访问模型的任意层。\n","#这就像模型是一个列表一样。每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数。\n","print(net[2].state_dict())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["OrderedDict([('weight', tensor([[ 0.1150, -0.2016,  0.3099,  0.2008,  0.3167,  0.2360, -0.1325,  0.2955]])), ('bias', tensor([-0.1821]))])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhJGekKhaO2y","executionInfo":{"status":"ok","timestamp":1619310638760,"user_tz":-480,"elapsed":3871,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"80fd4511-e44a-4c47-e2f1-13b59026ac38"},"source":["#5.2.1.1. 目标参数\n","#注意，每个参数都表示为参数（parameter）类的一个实例。要对参数执行任何操作，首先我们需要访问底层的数值。\n","#有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。下面的代码从第二个神经网络层提取偏置，提取后返回的是一个参数类实例，并进一步访问该参数的值。\n","print(type(net[2].bias))\n","print(net[2].bias)\n","print(net[2].bias.data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'torch.nn.parameter.Parameter'>\n","Parameter containing:\n","tensor([-0.1821], requires_grad=True)\n","tensor([-0.1821])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IR6NEOcZauaF","executionInfo":{"status":"ok","timestamp":1619310638761,"user_tz":-480,"elapsed":3870,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"203b1463-b803-4609-f9cb-8f30125de381"},"source":["#除了值之外，我们还可以访问每个参数的梯度。由于我们还没有调用这个网络的反向传播，所以参数的梯度处于初始状态。\n","net[2].weight.grad == None"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0xL18G0ay-I","executionInfo":{"status":"ok","timestamp":1619310638761,"user_tz":-480,"elapsed":3868,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"e1d9fc29-8571-4642-ee9b-0105e92fd3da"},"source":["#5.2.1.2. 一次性访问所有参数\n","#当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。当我们处理更复杂的块（例如，嵌套块）时，\n","#情况可能会变得特别复杂，因为我们需要递归整个树来提取每个子块的参数。下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n","print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n","print(*[(name, param.shape) for name, param in net.named_parameters()])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n","('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZFKMr6Ja5YJ","executionInfo":{"status":"ok","timestamp":1619310638762,"user_tz":-480,"elapsed":3867,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"d5532d7d-53fb-4a0a-af6f-cce56649935c"},"source":["#这为我们提供了另一种访问网络参数的方式，如下所示。\n","net.state_dict()['2.bias'].data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.1821])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7LM3G7Ba_jv","executionInfo":{"status":"ok","timestamp":1619310638762,"user_tz":-480,"elapsed":3863,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"cd33c001-ae59-4935-cbcf-de4e82431510"},"source":["#5.2.1.3. 从嵌套块收集参数\n","#让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。为此，\n","#我们首先定义一个生成块的函数（可以说是块工厂），然后将这些块组合到更大的块中。\n","def block1():\n","    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())\n","\n","def block2():\n","    net = nn.Sequential()\n","    for i in range(4):\n","        # 在这里嵌套\n","        net.add_module(f'block {i}', block1())\n","    return net\n","\n","rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n","rgnet(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3288],\n","        [0.3288]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZ4j8mlUbFUx","executionInfo":{"status":"ok","timestamp":1619310638762,"user_tz":-480,"elapsed":3861,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"04fc3b91-c8a8-4fd6-9664-daea5653171e"},"source":["#现在我们已经设计了网络，让我们看看它是如何组织的。\n","print(rgnet)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sequential(\n","  (0): Sequential(\n","    (block 0): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 1): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 2): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 3): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","  )\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbqpAETAbK0u","executionInfo":{"status":"ok","timestamp":1619310638763,"user_tz":-480,"elapsed":3861,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"4c4a5d21-4067-41e5-b7c2-b5621ec8b106"},"source":["#因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。例如，我们下面访问第一个主要的块，其中第二个子块的第一层的偏置项\n","rgnet[0][1][0].bias.data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.1530,  0.0529, -0.0518, -0.3935,  0.3749, -0.3026,  0.0008, -0.0241])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfOuDOL9bWHq","executionInfo":{"status":"ok","timestamp":1619310638763,"user_tz":-480,"elapsed":3859,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"cb3d6fdc-6825-4851-f572-0e22b7697054"},"source":["#5.2.2.1. 内置初始化\n","#让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。\n","def init_normal(m):\n","    if type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, mean=0, std=0.01)\n","        nn.init.zeros_(m.bias)\n","\n","net.apply(init_normal)\n","net[0].weight.data[0], net[0].bias.data[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 0.0103, -0.0128, -0.0003, -0.0064]), tensor(0.))"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODNWx63_bayx","executionInfo":{"status":"ok","timestamp":1619310638764,"user_tz":-480,"elapsed":3857,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"d6665391-f980-40cb-b608-3c32556ff85d"},"source":["#我们还可以将所有参数初始化为给定的常数（比如1）。\n","def init_constant(m):\n","    if type(m) == nn.Linear:\n","        nn.init.constant_(m.weight, 1)\n","        nn.init.zeros_(m.bias)\n","\n","net.apply(init_constant)\n","net[0].weight.data[0], net[0].bias.data[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1., 1., 1., 1.]), tensor(0.))"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqEB-c7KbeaF","executionInfo":{"status":"ok","timestamp":1619310638764,"user_tz":-480,"elapsed":3855,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"f16ebbf3-c682-4bd1-fdd0-9c7d8c731c80"},"source":["#我们还可以对某些块应用不同的初始化方法。例如，下面我们使用Xavier初始化方法初始化第一层，\n","#然后第二层初始化为常量值42。\n","def xavier(m):\n","    if type(m) == nn.Linear:\n","        nn.init.xavier_uniform_(m.weight)\n","\n","def init_42(m):\n","    if type(m) == nn.Linear:\n","        nn.init.constant_(m.weight, 42)\n","\n","net[0].apply(xavier)\n","net[2].apply(init_42)\n","print(net[0].weight.data[0])\n","print(net[2].weight.data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-0.1987, -0.1563, -0.3111,  0.5019])\n","tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKcx9vqhbnvW","executionInfo":{"status":"ok","timestamp":1619310638765,"user_tz":-480,"elapsed":3854,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"5e480da2-2283-4cde-f530-52b9e9ad3893"},"source":["#5.2.2.2. 自定义初始化\n","#有时，深度学习框架没有提供我们需要的初始化方法。在下面的例子中，\n","#我们使用以下的分布为任意权重参数 w 定义初始化方法：\n","def my_init(m):\n","    if type(m) == nn.Linear:\n","        print(\n","            \"Init\",\n","            *[(name, param.shape) for name, param in m.named_parameters()][0])\n","        nn.init.uniform_(m.weight, -10, 10)\n","        m.weight.data *= m.weight.data.abs() >= 5\n","\n","net.apply(my_init)\n","net[0].weight[:2]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Init weight torch.Size([8, 4])\n","Init weight torch.Size([1, 8])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.0000, -0.0000, -0.0000,  8.5588],\n","        [-8.5667, -0.0000,  0.0000,  5.1567]], grad_fn=<SliceBackward>)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Tm7ixUKbt-D","executionInfo":{"status":"ok","timestamp":1619310638765,"user_tz":-480,"elapsed":3852,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"ca912641-b330-4a11-c9f0-4333ee453343"},"source":["#注意，我们始终可以直接设置参数。\n","net[0].weight.data[:] += 1\n","net[0].weight.data[0, 0] = 42\n","net[0].weight.data[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([42.0000,  1.0000,  1.0000,  9.5588])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BB5Osk6obwjl","executionInfo":{"status":"ok","timestamp":1619310638765,"user_tz":-480,"elapsed":3850,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"7dddc490-6d09-4867-e7f0-533796652d7b"},"source":["#5.2.3. 参数绑定\n","#有时我们希望在多个层间共享参数。让我们看看如何优雅地做这件事。在下面，我们定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n","# 我们需要给共享层一个名称，以便可以引用它的参数。\n","shared = nn.Linear(8, 8)\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,\n","                    nn.ReLU(), nn.Linear(8, 1))\n","net(X)\n","# 检查参数是否相同\n","print(net[2].weight.data[0] == net[4].weight.data[0])\n","net[2].weight.data[0, 0] = 100\n","# 我们需要给共享层一个名称，以便可以引用它的参数。\n","print(net[2].weight.data[0] == net[4].weight.data[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([True, True, True, True, True, True, True, True])\n","tensor([True, True, True, True, True, True, True, True])\n"],"name":"stdout"}]}]}