{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"13.6. 目标检测数据集.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPrZSRL23/z4sTnMxkxVaoO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"LRvLOzgPzO3W"},"source":["#目标检测领域没有像 MNIST 和 Fashion-MNIST 那样的小数据集。 \n","#为了快速测试目标检测模型，我们收集并标记了一个小型数据集。 首先，我们拍摄了一组香蕉的照片，并生成了 1000 张不同角度和大小的香蕉图像。 \n","#然后，我们在一些背景图片的随机位置上放一张香蕉的图像。 最后，我们在图片上为这些香蕉标记了边界框。"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JJ0VFkCzcQI"},"source":["#13.6.1. 下载数据集\n","#包含所有图像和 csv 标签文件的香蕉检测数据集可以直接从互联网下载。\n","!pip install d2l\n","%matplotlib inline\n","import os\n","import pandas as pd\n","import torch\n","import torchvision\n","from d2l import torch as d2l\n","\n","d2l.DATA_HUB['banana-detection'] = (\n","    d2l.DATA_URL + 'banana-detection.zip',\n","    '5de26c8fce5ccdea9f91267273464dc968d20d72')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sHm_XIwznVF"},"source":["#13.6.2. 读取数据集¶\n","#通过 read_data_bananas 函数，我们读取香蕉检测数据集。 \n","#该数据集包括一个的 csv 文件，内含目标类别标签和位于左上角和右下角的真实边界框坐标。\n","def read_data_bananas(is_train=True):\n","    \"\"\"读取香蕉检测数据集中的图像和标签。\"\"\"\n","    data_dir = d2l.download_extract('banana-detection')\n","    csv_fname = os.path.join(data_dir,\n","                             'bananas_train' if is_train else 'bananas_val',\n","                             'label.csv')\n","    csv_data = pd.read_csv(csv_fname)\n","    csv_data = csv_data.set_index('img_name')\n","    images, targets = [], []\n","    for img_name, target in csv_data.iterrows():\n","        images.append(\n","            torchvision.io.read_image(\n","                os.path.join(data_dir,\n","                             'bananas_train' if is_train else 'bananas_val',\n","                             'images', f'{img_name}')))\n","        # Here `target` contains (class, upper-left x, upper-left y,\n","        # lower-right x, lower-right y), where all the images have the same\n","        # banana class (index 0)\n","        targets.append(list(target))\n","    return images, torch.tensor(targets).unsqueeze(1) / 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpuWjkoBzueo"},"source":["#通过使用 read_data_bananas 函数读取图像和标签，以下 BananasDataset 类别将允许我们创建一个自定义 Dataset 实例来加载香蕉检测数据集。\n","class BananasDataset(torch.utils.data.Dataset):\n","    \"\"\"一个用于加载香蕉检测数据集的自定义数据集。\"\"\"\n","    def __init__(self, is_train):\n","        self.features, self.labels = read_data_bananas(is_train)\n","        print('read ' + str(len(self.features)) + (\n","            f' training examples' if is_train else f' validation examples'))\n","\n","    def __getitem__(self, idx):\n","        return (self.features[idx].float(), self.labels[idx])\n","\n","    def __len__(self):\n","        return len(self.features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BbKxuSFznWP"},"source":["#最后，我们定义 load_data_bananas 函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无需按随机顺序读取它。\n","def load_data_bananas(batch_size):\n","    \"\"\"加载香蕉检测数据集。\"\"\"\n","    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),\n","                                             batch_size, shuffle=True)\n","    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),\n","                                           batch_size)\n","    return train_iter, val_iter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUIk2FYxz6ef"},"source":["batch_size, edge_size = 32, 256\n","train_iter, _ = load_data_bananas(batch_size)\n","batch = next(iter(train_iter))\n","batch[0].shape, batch[1].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ab63nDlEz-3p"},"source":["#13.6.3. 示范\n","#让我们展示 10 幅带有真实边界框的图像。 \n","#我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。 \n","#当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多\n","imgs = (batch[0][0:10].permute(0, 2, 3, 1)) / 255\n","axes = d2l.show_images(imgs, 2, 5, scale=2)\n","for ax, label in zip(axes, batch[1][0:10]):\n","    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])"],"execution_count":null,"outputs":[]}]}