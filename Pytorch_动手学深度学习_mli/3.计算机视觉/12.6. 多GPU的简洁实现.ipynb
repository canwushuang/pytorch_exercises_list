{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12.6. 多GPU的简洁实现.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOgEWSJgG7iFCn8S2uBgobl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"wYFPHkwClj9W"},"source":["#每个新模型的并行计算都从零开始实现是无趣的。\n","#此外，优化同步工具以获得高性能也是有好处的。下面我们将展示如何使用深度学习框架的高级 API 来实现这一点。\n","#数学和算法与 12.5节 中的相同。不出所料，你至少需要两个 GPU 来运行本节的代码。\n","!pip install d2l\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOju_7MilvMQ"},"source":["#12.6.1. 简单网络\n","#让我们使用一个比 12.5节 的 LeNet 更有意义的网络，它依然能够容易地和快速地训练。\n","#我们选择的是 [He et al., 2016a] 中的 ResNet-18。因为输入的图像很小，所以稍微修改了一下。与 7.6节 的区别在于，\n","#我们在开始时使用了更小的卷积核、步长和填充，而且删除了最大池化层\n","def resnet18(num_classes, in_channels=1):\n","    \"\"\"稍加修改的 ResNet-18 模型。\"\"\"\n","    def resnet_block(in_channels, out_channels, num_residuals,\n","                     first_block=False):\n","        blk = []\n","        for i in range(num_residuals):\n","            if i == 0 and not first_block:\n","                blk.append(\n","                    d2l.Residual(in_channels, out_channels, use_1x1conv=True,\n","                                 strides=2))\n","            else:\n","                blk.append(d2l.Residual(out_channels, out_channels))\n","        return nn.Sequential(*blk)\n","\n","    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大池化层。\n","    net = nn.Sequential(\n","        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(64), nn.ReLU())\n","    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n","    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n","    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n","    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n","    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1, 1)))\n","    net.add_module(\"fc\",\n","                   nn.Sequential(nn.Flatten(), nn.Linear(512, num_classes)))\n","    return net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJvaDhuOlvNd"},"source":["#12.6.2. 网络初始化\n","#我们将在训练回路中初始化网络。请参见 4.8节 复习初始化方法。\n","net = resnet18(10)\n","# 获取GPU列表\n","devices = d2l.try_all_gpus()\n","# 我们将在训练代码实现中初始化网络"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4xcNQ9jl-YH"},"source":["#12.6.3. 训练\n","#如前所述，用于训练的代码需要执行几个基本功能才能实现高效并行：\n","  #需要在所有设备上初始化网络参数。\n","  #在数据集上迭代时，要将小批量数据分配到所有设备上。\n","  #跨设备并行计算损失及其梯度。\n","  #聚合梯度，并相应地更新参数。\n","#最后，并行地计算精确度和发布网络的最终性能。除了需要拆分和聚合数据外，训练代码与前几章的实现非常相似。\n","def train(net, num_gpus, batch_size, lr):\n","    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n","    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n","\n","    def init_weights(m):\n","        if type(m) in [nn.Linear, nn.Conv2d]:\n","            nn.init.normal_(m.weight, std=0.01)\n","\n","    net.apply(init_weights)\n","    # 在多个 GPU 上设置模型\n","    net = nn.DataParallel(net, device_ids=devices)\n","    trainer = torch.optim.SGD(net.parameters(), lr)\n","    loss = nn.CrossEntropyLoss()\n","    timer, num_epochs = d2l.Timer(), 10\n","    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n","    for epoch in range(num_epochs):\n","        net.train()\n","        timer.start()\n","        for X, y in train_iter:\n","            trainer.zero_grad()\n","            X, y = X.to(devices[0]), y.to(devices[0])\n","            l = loss(net(X), y)\n","            l.backward()\n","            trainer.step()\n","        timer.stop()\n","        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n","    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n","          f'on {str(devices)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyZLYxrMmGvM"},"source":["#让我们看看这在实践中是如何运作的。我们先在单个GPU上训练网络进行预热。\n","train(net, num_gpus=1, batch_size=256, lr=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpBPPnfDmMDj","executionInfo":{"status":"ok","timestamp":1624195531700,"user_tz":-480,"elapsed":3,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}}},"source":["#接下来我们使用 2 个 GPU 进行训练。与 12.5节 中评估的 LeNet 相比，ResNet-18 的模型要复杂得多。\n","#这就是显示并行化优势的地方，计算所需时间明显大于同步参数需要的时间。因为并行化开销的相关性较小，因此这种操作提高了模型的可伸缩性。\n","train(net, num_gpus=2, batch_size=512, lr=0.2)"],"execution_count":1,"outputs":[]}]}