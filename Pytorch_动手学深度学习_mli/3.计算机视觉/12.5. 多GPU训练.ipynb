{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12.5. 多GPU训练.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM7a1J3xbdfywcV/sAHIYL+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"tnmhd6Qhjgkb"},"source":["!pip install d2l\n","%matplotlib inline\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJ-5FEBbkF3S"},"source":["#12.5.3. 简单网络\n","#我们使用 6.6节 中介绍的（稍加修改的） LeNet， 从零开始定义它，从而详细说明参数交换和同步。\n","# 初始化模型参数\n","scale = 0.01\n","W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n","b1 = torch.zeros(20)\n","W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n","b2 = torch.zeros(50)\n","W3 = torch.randn(size=(800, 128)) * scale\n","b3 = torch.zeros(128)\n","W4 = torch.randn(size=(128, 10)) * scale\n","b4 = torch.zeros(10)\n","params = [W1, b1, W2, b2, W3, b3, W4, b4]\n","\n","# 定义模型\n","def lenet(X, params):\n","    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n","    h1_activation = F.relu(h1_conv)\n","    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n","    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n","    h2_activation = F.relu(h2_conv)\n","    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n","    h2 = h2.reshape(h2.shape[0], -1)\n","    h3_linear = torch.mm(h2, params[4]) + params[5]\n","    h3 = F.relu(h3_linear)\n","    y_hat = torch.mm(h3, params[6]) + params[7]\n","    return y_hat\n","\n","# 交叉熵损失函数\n","loss = nn.CrossEntropyLoss(reduction='none')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcrqmBTVkKjE"},"source":["#12.5.4. 数据同步¶\n","#对于高效的多 GPU 训练，我们需要两个基本操作。 首先，我们需要 向多个设备分发参数 并附加梯度（get_params）。 \n","#如果没有参数，就不可能在 GPU 上评估网络。 第二，需要跨多个设备对参数求和，也就是说，需要一个 allreduce 函数。\n","def get_params(params, device):\n","    new_params = [p.clone().to(device) for p in params]\n","    for p in new_params:\n","        p.requires_grad_()\n","    return new_params"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMgRpHH_kahF"},"source":["#通过将模型参数复制到一个GPU。\n","new_params = get_params(params, d2l.try_gpu(0))\n","print('b1 weight:', new_params[1])\n","print('b1 grad:', new_params[1].grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hTgSTsokeAz"},"source":["#由于还没有进行任何计算，因此偏置参数的梯度仍然为零。 \n","#假设现在有一个向量分布在多个 GPU 上，下面的 allreduce 函数将所有向量相加，并将结果广播给所有 GPU。 \n","#请注意，我们需要将数据复制到累积结果的设备，才能使函数正常工作。\n","def allreduce(data):\n","    for i in range(1, len(data)):\n","        data[0][:] += data[i].to(data[0].device)\n","    for i in range(1, len(data)):\n","        data[i] = data[0].to(data[i].device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"urAydboIkjH_"},"source":["#通过在不同设备上创建具有不同值的向量并聚合它们。\n","data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\n","print('before allreduce:\\n', data[0], '\\n', data[1])\n","allreduce(data)\n","print('after allreduce:\\n', data[0], '\\n', data[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujBaUC9jktRQ"},"source":["#12.5.5. 数据分发\n","#我们需要一个简单的工具函数，将一个小批量数据均匀地分布在多个 GPU 上。 \n","#例如，有两个 GPU 时，我们希望每个 GPU 可以复制一半的数据。 \n","#因为深度学习框架的内置函数编写代码更方便、更简洁，所以在  4×5  矩阵上使用它进行尝试。\n","data = torch.arange(20).reshape(4, 5)\n","devices = [torch.device('cuda:0'), torch.device('cuda:1')]\n","split = nn.parallel.scatter(data, devices)\n","print('input :', data)\n","print('load into', devices)\n","print('output:', split)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fem3uMZHk08-"},"source":["#为了方便以后复用，我们定义了可以同时拆分数据和标签的 split_batch 函数。\n","def split_batch(X, y, devices):\n","    \"\"\"将`X`和`y`拆分到多个设备上\"\"\"\n","    assert X.shape[0] == y.shape[0]\n","    return (nn.parallel.scatter(X, devices), nn.parallel.scatter(y, devices))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFcf6oNsk692"},"source":["#12.5.6. 训练\n","#现在我们可以 在一个小批量上实现多 GPU 训练。 \n","#在多个 GPU 之间同步数据将使用刚才讨论的辅助函数 allreduce 和 split_and_load。 \n","#我们不需要编写任何特定的代码来实现并行性。 因为计算图在小批量内的设备之间没有任何依赖关系，因此它是“自动地”并行执行。\n","def train_batch(X, y, device_params, devices, lr):\n","    X_shards, y_shards = split_batch(X, y, devices)\n","    # 在每个GPU上分别计算损失\n","    ls = [\n","        loss(lenet(X_shard, device_W),\n","             y_shard).sum() for X_shard, y_shard, device_W in zip(\n","                 X_shards, y_shards, device_params)]\n","    for l in ls:  # 反向传播在每个GPU上分别执行\n","        l.backward()\n","    # 将每个GPU的所有梯度相加，并将其广播到所有GPU\n","    with torch.no_grad():\n","        for i in range(len(device_params[0])):\n","            allreduce([device_params[c][i].grad for c in range(len(devices))])\n","    # 在每个GPU上分别更新模型参数\n","    for param in device_params:\n","        d2l.sgd(param, lr, X.shape[0])  # 在这里，我们使用全尺寸的小批量"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCdcxtuJlBqh"},"source":["#现在，我们可以 定义训练函数。 \n","#与前几章中略有不同：训练函数需要分配 GPU 并将所有模型参数复制到所有设备。 显然，每个小批量都是使用 train_batch 函数来处理多个GPU。 \n","#我们只在一个 GPU 上计算模型的精确度，而让其他 GPU 保持空闲，尽管这是相对低效的，但是使用方便且代码简洁。\n","def train(num_gpus, batch_size, lr):\n","    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n","    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n","    # 将模型参数复制到`num_gpus`个GPU\n","    device_params = [get_params(params, d) for d in devices]\n","    num_epochs = 10\n","    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n","    timer = d2l.Timer()\n","    for epoch in range(num_epochs):\n","        timer.start()\n","        for X, y in train_iter:\n","            # 为单个小批量执行多GPU训练\n","            train_batch(X, y, device_params, devices, lr)\n","            torch.cuda.synchronize()\n","        timer.stop()\n","        # 在GPU 0上评估模型\n","        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n","            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n","    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n","          f'on {str(devices)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsWvPGMglGj5"},"source":["#让我们看看 在单个GPU上运行 效果得有多好。 首先使用的批量大小是  256 ，学习率是  0.2 。\n","train(num_gpus=1, batch_size=256, lr=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nQlWX4alKkg"},"source":["#保持批量大小和学习率不变，并 增加为2个GPU，我们可以看到测试精度与之前的实验基本相同。 \n","#不同的 GPU 个数在算法寻优方面是相同的。 不幸的是，这里没有任何有意义的加速：模型实在太小了；\n","#而且数据集也太小了，在这个数据集中，我们实现的多 GPU 训练的简单方法受到了巨大的 Python 开销的影响。 \n","#在未来，我们将遇到更复杂的模型和更复杂的并行化方法。 尽管如此，让我们看看 Fashion-MNIST 数据集上会发生什么。\n","train(num_gpus=2, batch_size=256, lr=0.2)"],"execution_count":null,"outputs":[]}]}