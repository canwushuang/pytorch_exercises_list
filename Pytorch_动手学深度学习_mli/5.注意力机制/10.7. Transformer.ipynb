{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.7. Transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMWgwDo76kUm+q+FoP3KK8I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"jTEi_fmDwa7L"},"source":["#10.7.1. 模型\n","#我们已经描述并实现了基于缩放点积多头注意力 10.5节 和位置编码 10.6.3节 。接下来，我们将实现 Transformer 模型的剩余部分。\n","!pip install d2l\n","import math\n","import pandas as pd\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTPb2DfKwz55"},"source":["#10.7.2. 基于位置的前馈网络\n","#基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），\n","#这就是称前馈网络是 基于位置的（positionwise）的原因。\n","#在下面的实现中，输入 X 的形状（批量大小、时间步数或序列长度、隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小、时间步数、ffn_num_outputs）的输出张量。\n","class PositionWiseFFN(nn.Module):\n","    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n","                 **kwargs):\n","        super(PositionWiseFFN, self).__init__(**kwargs)\n","        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n","        self.relu = nn.ReLU()\n","        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n","\n","    def forward(self, X):\n","        return self.dense2(self.relu(self.dense1(X)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQzupzPlw9g1"},"source":["#下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。\n","#因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。\n","ffn = PositionWiseFFN(4, 4, 8)\n","ffn.eval()\n","ffn(torch.ones((2, 3, 4)))[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueBk_H2xxEfG"},"source":["#10.7.3. 残差连接和层归一化\n","#以下代码对比不同维度的层归一化和批量归一化的效果。\n","ln = nn.LayerNorm(2)\n","bn = nn.BatchNorm1d(2)\n","X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n","# 在训练模式下计算 `X` 的均值和方差\n","print('layer norm:', ln(X), '\\nbatch norm:', bn(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Yq3bvVyxLBW"},"source":["#现在我们可以使用残差连接和层归一化来实现 AddNorm 类。Dropout 也被作为正则化方法使用。\n","class AddNorm(nn.Module):\n","    def __init__(self, normalized_shape, dropout, **kwargs):\n","        super(AddNorm, self).__init__(**kwargs)\n","        self.dropout = nn.Dropout(dropout)\n","        self.ln = nn.LayerNorm(normalized_shape)\n","\n","    def forward(self, X, Y):\n","        return self.ln(self.dropout(Y) + X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liDEa3WJxNM2"},"source":["#残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。\n","add_norm = AddNorm([3, 4], 0.5) # Normalized_shape is input.size()[1:]\n","add_norm.eval()\n","add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMlkJaK-xP9e"},"source":["#10.7.4. 编码器\n","#有了组成 Transformer 编码器的基础组件，现在可以先实现编码器中的一个层。\n","#下面的 EncoderBlock 类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层归一化。\n","class EncoderBlock(nn.Module):\n","    def __init__(self, key_size, query_size, value_size, num_hiddens,\n","                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n","                 dropout, use_bias=False, **kwargs):\n","        super(EncoderBlock, self).__init__(**kwargs)\n","        self.attention = d2l.MultiHeadAttention(\n","            key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n","            use_bias)\n","        self.addnorm1 = AddNorm(norm_shape, dropout)\n","        self.ffn = PositionWiseFFN(\n","            ffn_num_input, ffn_num_hiddens, num_hiddens)\n","        self.addnorm2 = AddNorm(norm_shape, dropout)\n","\n","    def forward(self, X, valid_lens):\n","        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n","        return self.addnorm2(Y, self.ffn(Y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6xfuPAqxXAB"},"source":["#正如我们所看到的，Transformer编码器中的任何层都不会改变其输入的形状。\n","X = torch.ones((2, 100, 24))\n","valid_lens = torch.tensor([3, 2])\n","encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)\n","encoder_blk.eval()\n","encoder_blk(X, valid_lens).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOgbhaeExauR"},"source":["#在实现下面的Transformer编码器的代码中，我们堆叠了 num_layers 个 EncoderBlock 类的实例。\n","#由于我们使用的是值范围在  −1  和  1  之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。\n","class TransformerEncoder(d2l.Encoder):\n","    def __init__(self, vocab_size, key_size, query_size, value_size,\n","                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n","                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.num_hiddens = num_hiddens\n","        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n","        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n","        self.blks = nn.Sequential()\n","        for i in range(num_layers):\n","            self.blks.add_module(\"block\"+str(i),\n","                EncoderBlock(key_size, query_size, value_size, num_hiddens,\n","                             norm_shape, ffn_num_input, ffn_num_hiddens,\n","                             num_heads, dropout, use_bias))\n","\n","    def forward(self, X, valid_lens, *args):\n","        # 因为位置编码值在 -1 和 1 之间，\n","        # 因此嵌入值乘以嵌入维度的平方根进行缩放，\n","        # 然后再与位置编码相加。\n","        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n","        self.attention_weights = [None] * len(self.blks)\n","        for i, blk in enumerate(self.blks):\n","            X = blk(X, valid_lens)\n","            self.attention_weights[\n","                i] = blk.attention.attention.attention_weights\n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usXV9M1mzgxw"},"source":["#下面我们指定了超参数来创建一个两层的Transformer编码器。\n","#Transformer 编码器输出的形状是（批量大小、时间步的数目、num_hiddens）。\n","encoder = TransformerEncoder(200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n","encoder.eval()\n","encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49bGscgQzlKg"},"source":["#10.7.5. 解码器\n","class DecoderBlock(nn.Module):\n","    \"\"\"解码器中第 i 个块\"\"\"\n","    def __init__(self, key_size, query_size, value_size, num_hiddens,\n","                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n","                 dropout, i, **kwargs):\n","        super(DecoderBlock, self).__init__(**kwargs)\n","        self.i = i\n","        self.attention1 = d2l.MultiHeadAttention(\n","            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n","        self.addnorm1 = AddNorm(norm_shape, dropout)\n","        self.attention2 = d2l.MultiHeadAttention(\n","            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n","        self.addnorm2 = AddNorm(norm_shape, dropout)\n","        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,\n","                                   num_hiddens)\n","        self.addnorm3 = AddNorm(norm_shape, dropout)\n","\n","    def forward(self, X, state):\n","        enc_outputs, enc_valid_lens = state[0], state[1]\n","        # 训练阶段，输出序列的所有词元都在同一时间处理，\n","        # 因此 `state[2][self.i]` 初始化为 `None`。\n","        # 预测阶段，输出序列是通过词元一个接着一个解码的，\n","        # 因此 `state[2][self.i]` 包含着直到当前时间步第 `i` 个块解码的输出表示\n","        if state[2][self.i] is None:\n","            key_values = X\n","        else:\n","            key_values = torch.cat((state[2][self.i], X), axis=1)\n","        state[2][self.i] = key_values\n","        if self.training:\n","            batch_size, num_steps, _ = X.shape\n","            # `dec_valid_lens` 的开头: (`batch_size`, `num_steps`),\n","            # 其中每一行是 [1, 2, ..., `num_steps`]\n","            dec_valid_lens = torch.arange(\n","                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n","        else:\n","            dec_valid_lens = None\n","\n","        # 自注意力\n","        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n","        Y = self.addnorm1(X, X2)\n","        # 编码器－解码器注意力。\n","        # `enc_outputs` 的开头: (`batch_size`, `num_steps`, `num_hiddens`)\n","        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n","        Z = self.addnorm2(Y, Y2)\n","        return self.addnorm3(Z, self.ffn(Z)), state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EwjrhR6bzov4"},"source":["#为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是num_hiddens。\n","decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)\n","decoder_blk.eval()\n","X = torch.ones((2, 100, 24))\n","state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n","decoder_blk(X, state)[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Go2-MKphztH6"},"source":["#现在我们构建了由 num_layers 个 DecoderBlock 实例组成的完整的Transformer解码器。最后，通过一个全连接层计算所有 vocab_size 个可能的输出词元的预测值。\n","#解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。\n","class TransformerDecoder(d2l.AttentionDecoder):\n","    def __init__(self, vocab_size, key_size, query_size, value_size,\n","                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n","                 num_heads, num_layers, dropout, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.num_hiddens = num_hiddens\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n","        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n","        self.blks = nn.Sequential()\n","        for i in range(num_layers):\n","            self.blks.add_module(\"block\"+str(i),\n","                DecoderBlock(key_size, query_size, value_size, num_hiddens,\n","                             norm_shape, ffn_num_input, ffn_num_hiddens,\n","                             num_heads, dropout, i))\n","        self.dense = nn.Linear(num_hiddens, vocab_size)\n","\n","    def init_state(self, enc_outputs, enc_valid_lens, *args):\n","        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n","\n","    def forward(self, X, state):\n","        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n","        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n","        for i, blk in enumerate(self.blks):\n","            X, state = blk(X, state)\n","            # 解码器自注意力权重\n","            self._attention_weights[0][\n","                i] = blk.attention1.attention.attention_weights\n","            # “编码器－解码器”自注意力权重\n","            self._attention_weights[1][\n","                i] = blk.attention2.attention.attention_weights\n","        return self.dense(X), state\n","\n","    @property\n","    def attention_weights(self):\n","        return self._attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2OoOIGEzyAr"},"source":["#10.7.6. 训练\n","#依照 Transformer 结构来实例化编码器－解码器模型。\n","#在这里，指定 Transformer 的编码器和解码器都是 2 层，都使用 4 头注意力。与 9.7.4节 类似，为了进行序列到序列的学习，我们在“英语－法语”机器翻译数据集上训练 Transformer 模型。\n","num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10\n","lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n","ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n","key_size, query_size, value_size = 32, 32, 32\n","norm_shape = [32]\n","\n","train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n","\n","encoder = TransformerEncoder(\n","    len(src_vocab), key_size, query_size, value_size, num_hiddens,\n","    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n","    num_layers, dropout)\n","decoder = TransformerDecoder(\n","    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,\n","    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n","    num_layers, dropout)\n","net = d2l.EncoderDecoder(encoder, decoder)\n","d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVcXTIASz2fy"},"source":["#训练结束后，使用 Transformer 模型将一些英语句子翻译成法语，并且计算它们的 BLEU 分数。\n","engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n","fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n","for eng, fra in zip(engs, fras):\n","    translation, dec_attention_weight_seq = d2l.predict_seq2seq(\n","        net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n","    print(f'{eng} => {translation}, ',\n","          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyMV1wbtz7hl"},"source":["#当进行最后一个英语到法语的句子翻译工作时，让我们可视化Transformer 的注意力权重。编码器自注意力权重的形状为 (编码器层数, 注意力头数, num_steps或查询的数目, num_steps 或“键－值”对的数目) 。\n","enc_attention_weights = torch.cat(net.encoder.attention_weights, 0).reshape((num_layers, num_heads,\n","    -1, num_steps))\n","enc_attention_weights.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6u-Iz2ftz_d1"},"source":["#在编码器的自注意力中，查询和键都来自相同的输入序列。\n","#因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。\n","#接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。\n","d2l.show_heatmaps(\n","    enc_attention_weights.cpu(), xlabel='Key positions',\n","    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\n","    figsize=(7, 3.5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfNM_IS-3p7p"},"source":["dec_attention_weights_2d = [np.array(head[0]).tolist()\n","                            for step in dec_attention_weight_seq\n","                            for attn in step for blk in attn for head in blk]\n","dec_attention_weights_filled = np.array(\n","    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)\n","dec_attention_weights = dec_attention_weights_filled.reshape((-1, 2, num_layers, num_heads, num_steps))\n","dec_self_attention_weights, dec_inter_attention_weights = \\\n","    dec_attention_weights.transpose(1, 2, 3, 0, 4)\n","dec_self_attention_weights.shape, dec_inter_attention_weights.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BhoMadF3qSH"},"source":["# Plus one to include the beginning-of-sequence token\n","d2l.show_heatmaps(\n","    dec_self_attention_weights[:, :, :, :len(translation.split()) + 1],\n","    xlabel='Key positions', ylabel='Query positions',\n","    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0onaqD4H3s-H"},"source":["d2l.show_heatmaps(\n","    dec_inter_attention_weights, xlabel='Key positions',\n","    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\n","    figsize=(7, 3.5))"],"execution_count":null,"outputs":[]}]}