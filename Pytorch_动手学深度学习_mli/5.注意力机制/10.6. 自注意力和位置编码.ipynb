{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.6. 自注意力和位置编码.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+Va+4pI1TZaezQpL9skXf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"h_W9EwgdvlHT"},"source":["#在本节中，我们将讨论使用自注意力进行序列编码，包括使用序列的顺序作为补充信息。\n","!pip install d2l\n","import math\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sx7kPtDHvu_4"},"source":["#10.6.1. 自注意力\n","#根据 (10.2.4) 中定义的注意力池化函数  f 。\n","#下面的代码片段是基于多头注意力对一个张量完成自注意力的计算，张量的形状为 (批量大小, 时间步的数目或词元序列的长度,  d ) 。\n","#输出与输入的张量形状相同。\n","num_hiddens, num_heads = 100, 5\n","attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n","                                   num_hiddens, num_heads, 0.5)\n","attention.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2FPmoYWYvxPP"},"source":["batch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])\n","X = torch.ones((batch_size, num_queries, num_hiddens))\n","attention(X, X, X, valid_lens).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfO7jwPuv5fr"},"source":["#10.6.3. 位置编码\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, num_hiddens, dropout, max_len=1000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        # 创建一个足够长的 `P`\n","        self.P = torch.zeros((1, max_len, num_hiddens))\n","        X = torch.arange(max_len, dtype=torch.float32).reshape(\n","            -1, 1) / torch.pow(10000, torch.arange(\n","            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n","        self.P[:, :, 0::2] = torch.sin(X)\n","        self.P[:, :, 1::2] = torch.cos(X)\n","\n","    def forward(self, X):\n","        X = X + self.P[:, :X.shape[1], :].to(X.device)\n","        return self.dropout(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVSEEKDKv_PR"},"source":["#在位置嵌入矩阵  P  中，行代表词元在序列中的位置，列代表位置编码的不同维度。\n","#在下面的例子中，我们可以看到位置嵌入矩阵的 第 6 列 和 第 7 列的频率高于 第 8  列和 第 9  列。\n","#第 6 列 和 第 7 列之间的偏移量（第 8  列和 第 9  列相同）是由于正弦函数和余弦函数的交替。\n","encoding_dim, num_steps = 32, 60\n","pos_encoding = PositionalEncoding(encoding_dim, 0)\n","pos_encoding.eval()\n","X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))\n","P = pos_encoding.P[:, :X.shape[1], :]\n","d2l.plot(torch.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\n","         figsize=(6, 2.5), legend=[\"Col %d\" % d for d in torch.arange(6, 10)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Bq_zGkBwIeN"},"source":["#10.6.3.1. 绝对位置信息\n","#为了明白沿着编码维度单调降低的频率与绝对位置信息的关系，让我们打印出  0,1,…,7  \n","#的二进制表示形式。正如我们所看到的，每个数字、每两个数字和每四个数字上的比特值在第一个最低位、第二个最低位和第三个最低位上分别交替。\n","for i in range(8):\n","    print(f'{i} in binary is {i:>03b}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKyS0qSKwP5P"},"source":["#在二进制表示中，较高比特位的交替频率低于较低比特位，与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。\n","#由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。\n","P = P[0, :, :].unsqueeze(0).unsqueeze(0)\n","d2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\n","                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')"],"execution_count":null,"outputs":[]}]}