{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.2. 注意力汇聚：Nadaraya-Watson 核回归.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNmME7aQ4qrDw8FrUMkndX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"QqAdWZzGgWyZ"},"source":["#具体来说，1964 年提出的 Nadaraya-Watson 核回归模型是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。\n","!pip install d2l\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"whusn8RYgaQz"},"source":["#10.2.1. 生成数据集\n","#我们生成了  50  个训练样本和  50  个测试样本。为了更好地可视化之后的注意力模式，输入的训练样本将进行排序。\n","n_train = 50  # 训练样本数\n","x_train, _ = torch.sort(torch.rand(n_train) * 5)   # 训练样本的输入\n","\n","def f(x):\n","    return 2 * torch.sin(x) + x**0.8\n","\n","y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # 训练样本的输出\n","x_test = torch.arange(0, 5, 0.1)  # 测试样本\n","y_truth = f(x_test)  # 测试样本的真实输出\n","n_test = len(x_test)  # 测试样本数\n","n_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7a-hlPGgnva"},"source":["#下面的函数将绘制所有的训练样本（样本由圆圈表示）、不带噪声项的真实数据生成函数  f （标记为“Truth”）和学习得到的预测函数（标记为“Pred”）。\n","def plot_kernel_reg(y_hat):\n","    d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'], xlim=[0, 5], ylim=[-1, 5])\n","    d2l.plt.plot(x_train, y_train, 'o', alpha=0.5);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQlbdOhVgv7S"},"source":["#10.2.2. 平均汇聚\n","#先使用可能是这个世界上“最愚蠢”的估计器来解决回归问题：基于平均汇聚来计算所有训练样本输出值的平均值：\n","y_hat = torch.repeat_interleave(y_train.mean(), n_test)\n","plot_kernel_reg(y_hat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gcDmp0gg11q"},"source":["#10.2.3. 非参数注意力汇聚\n","# `X_repeat` 的形状: (`n_test`, `n_train`),\n","# 每一行都包含着相同的测试输入（例如：同样的查询）\n","X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))\n","# `x_train` 包含着键。`attention_weights` 的形状：(`n_test`, `n_train`),\n","# 每一行都包含着要在给定的每个查询的值（`y_train`）之间分配的注意力权重\n","attention_weights = nn.functional.softmax(-(X_repeat - x_train)**2 / 2, dim=1)\n","# `y_hat` 的每个元素都是值的加权平均值，其中的权重是注意力权重\n","y_hat = torch.matmul(attention_weights, y_train)\n","plot_kernel_reg(y_hat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0q8zfFpZg6_C"},"source":["#10.2.4.1. 批量矩阵乘法\n","#为了更有效地计算小批量数据的注意力，我们可以利用深度学习开发框架中提供的批量矩阵乘法。\n","X = torch.ones((2, 1, 4))\n","Y = torch.ones((2, 4, 6))\n","torch.bmm(X, Y).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUAzRydQg9Iq"},"source":["#在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。\n","weights = torch.ones((2, 10)) * 0.1\n","values = torch.arange(20.0).reshape((2, 10))\n","torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3FJ-5uug_wS"},"source":["#10.2.4.2. 定义模型\n","#基于 (10.2.7) 中的带参数的注意力汇聚，使用小批量矩阵乘法，定义 Nadaraya-Watson 核回归的带参数版本为：\n","class NWKernelRegression(nn.Module):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))\n","\n","    def forward(self, queries, keys, values):\n","        # `queries` 和 `attention_weights` 的形状为 (查询个数, “键－值”对个数)\n","        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))\n","        self.attention_weights = nn.functional.softmax(\n","            -((queries - keys) * self.w)**2 / 2, dim=1)\n","        # `values` 的形状为 (查询个数, “键－值”对个数)\n","        return torch.bmm(self.attention_weights.unsqueeze(1),\n","                         values.unsqueeze(-1)).reshape(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9DthYcUChFGL"},"source":["#10.2.4.3. 训练模型\n","#接下来，将训练数据集转换为键和值用于训练注意力模型。\n","#在带参数的注意力汇聚模型中，任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算，从而得到其对应的预测输出。\n","# `X_tile` 的形状: (`n_train`, `n_train`), 每一行都包含着相同的训练输入\n","X_tile = x_train.repeat((n_train, 1))\n","# `Y_tile` 的形状: (`n_train`, `n_train`), 每一行都包含着相同的训练输出\n","Y_tile = y_train.repeat((n_train, 1))\n","# `keys` 的形状: ('n_train', 'n_train' - 1)\n","keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))\n","# `values` 的形状: ('n_train', 'n_train' - 1)\n","values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1QXOuZnhIhr"},"source":["#训练带参数的注意力汇聚模型时使用平方损失函数和随机梯度下降。\n","net = NWKernelRegression()\n","loss = nn.MSELoss(reduction='none')\n","trainer = torch.optim.SGD(net.parameters(), lr=0.5)\n","animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])\n","\n","for epoch in range(5):\n","    trainer.zero_grad()\n","    # 注意：L2 Loss = 1/2 * MSE Loss。\n","    # PyTorch 的 MSE Loss 与 MXNet 的 L2Loss 差一个 2 的因子，因此被除2。\n","    l = loss(net(x_train, keys, values), y_train) / 2\n","    l.sum().backward()\n","    trainer.step()\n","    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')\n","    animator.add(epoch + 1, float(l.sum()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_eWJwxhhNb5"},"source":["#训练完带参数的注意力汇聚模型后，我们发现，在尝试拟合带噪声的训练数据时，预测结果绘制的线不如之前非参数模型的线平滑。\n","# `keys` 的形状: (`n_test`, `n_train`), 每一行包含着相同的训练输入（例如：相同的键）\n","keys = x_train.repeat((n_test, 1))\n","# `value` 的形状: (`n_test`, `n_train`)\n","values = y_train.repeat((n_test, 1))\n","y_hat = net(x_test, keys, values).unsqueeze(1).detach()\n","plot_kernel_reg(y_hat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSj77rEEhPI6"},"source":["#与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，在输出结果的绘制图上，曲线在注意力权重较大的区域变得更不平滑。\n","d2l.show_heatmaps(net.attention_weights.unsqueeze(0).unsqueeze(0),\n","                  xlabel='Sorted training inputs',\n","                  ylabel='Sorted testing inputs')"],"execution_count":null,"outputs":[]}]}