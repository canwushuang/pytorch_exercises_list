{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.5. 多头注意力.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNEyecMKI9gSwbpMn0DqwGP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"zzZJAgZymD75"},"source":["#10.5.1. 模型\n","!pip install d2l\n","import math\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"id":"tlOXxocJu-Ss","executionInfo":{"status":"error","timestamp":1628559948165,"user_tz":-480,"elapsed":1092,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"62d35827-be3d-4d3b-fb16-29aa2c21822b"},"source":["#10.5.2. 实现\n","#在实现过程中，我们选择缩放点积注意力作为每一个注意力头。\n","#为了避免计算成本和参数数量的大幅增长，我们设定  pq=pk=pv=po/h 。\n","#值得注意的是，如果我们将查询、键和值的线性变换的输出数量设置为  pqh=pkh=pvh=po ，则可以并行计算  h  个头。\n","#在下面的实现中， po  是通过参数 num_hiddens 指定的。\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, key_size, query_size, value_size, num_hiddens,\n","                 num_heads, dropout, bias=False, **kwargs):\n","        super(MultiHeadAttention, self).__init__(**kwargs)\n","        self.num_heads = num_heads\n","        self.attention = d2l.DotProductAttention(dropout)\n","        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n","        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n","        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n","        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n","\n","    def forward(self, queries, keys, values, valid_lens):\n","        # `queries`, `keys`, or `values` 的形状:\n","        # (`batch_size`, 查询或者“键－值”对的个数, `num_hiddens`)\n","        # `valid_lens`　的形状:\n","        # (`batch_size`,) or (`batch_size`, 查询的个数)\n","        # 经过变换后，输出的 `queries`, `keys`, or `values`　的形状:\n","        # (`batch_size` * `num_heads`, 查询或者“键－值”对的个数,\n","        # `num_hiddens` / `num_heads`)\n","        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n","        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n","        values = transpose_qkv(self.W_v(values), self.num_heads)\n","\n","        if valid_lens is not None:\n","            # 在轴 0，将第一项（标量或者矢量）复制 `num_heads` 次，\n","            # 然后如此复制第二项，然后诸如此类。\n","            valid_lens = torch.repeat_interleave(\n","                valid_lens, repeats=self.num_heads, dim=0)\n","\n","        # `output` 的形状: (`batch_size` * `num_heads`, 查询的个数,\n","        # `num_hiddens` / `num_heads`)\n","        output = self.attention(queries, keys, values, valid_lens)\n","\n","        # `output_concat` 的形状: (`batch_size`, 查询的个数, `num_hiddens`)\n","        output_concat = transpose_output(output, self.num_heads)\n","        return self.W_o(output_concat)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-79153b8460bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#在实现过程中，我们选择缩放点积注意力作为每一个注意力头。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#为了避免计算成本和参数数量的大幅增长，我们设定  pq=pk=pv=po/h 。值得注意的是，如果我们将查询、键和值的线性变换的输出数量设置为  pqh=pkh=pvh=po ，则可以并行计算  h  个头。在下面的实现中， po  是通过参数 num_hiddens 指定的。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     def __init__(self, key_size, query_size, value_size, num_hiddens,\n\u001b[1;32m      6\u001b[0m                  num_heads, dropout, bias=False, **kwargs):\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"aUEwm7imvIWr"},"source":["#为了能够使多个头并行计算，上面的 MultiHeadAttention 类使用了下面定义的两个转置函数。\n","#具体来说，transpose_output 函数反转了 transpose_qkv 函数的操作。\n","def transpose_qkv(X, num_heads):\n","    # 输入 `X` 的形状: (`batch_size`, 查询或者“键－值”对的个数, `num_hiddens`).\n","    # 输出 `X` 的形状: (`batch_size`, 查询或者“键－值”对的个数, `num_heads`,\n","    # `num_hiddens` / `num_heads`)\n","    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n","\n","    # 输出 `X` 的形状: (`batch_size`, `num_heads`, 查询或者“键－值”对的个数,\n","    # `num_hiddens` / `num_heads`)\n","    X = X.permute(0, 2, 1, 3)\n","\n","    # `output` 的形状: (`batch_size` * `num_heads`, 查询或者“键－值”对的个数,\n","    # `num_hiddens` / `num_heads`)\n","    return X.reshape(-1, X.shape[2], X.shape[3])\n","\n","def transpose_output(X, num_heads):\n","    \"\"\"逆转 `transpose_qkv` 函数的操作\"\"\"\n","    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n","    X = X.permute(0, 2, 1, 3)\n","    return X.reshape(X.shape[0], X.shape[1], -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kp1VdjMQvWmC"},"source":["#让我们使用键和值相同的小例子来测试我们编写的 MultiHeadAttention 类。\n","#多头注意力输出的形状是 (batch_size, num_queries, num_hiddens)。\n","num_hiddens, num_heads = 100, 5\n","attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n","                               num_hiddens, num_heads, 0.5)\n","attention.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4240i8r-vZf6"},"source":["batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])\n","X = torch.ones((batch_size, num_queries, num_hiddens))\n","Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n","attention(X, Y, Y, valid_lens).shape"],"execution_count":null,"outputs":[]}]}