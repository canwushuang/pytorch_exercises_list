{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.4. Bahdanau 注意力.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPUOpoIY62RWeHEwbtQXpuc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"q-gktmqFiWfu"},"source":["#10.4.1. 模型\n","!pip install d2l\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZuyMEf9if4K"},"source":["#10.4.2. 定义注意力解码器\n","#要用 Bahdanau 注意力实现循环神经网络编码器-解码器，我们只需重新定义解码器即可。\n","#为了更方便地显示学习的注意力权重，以下 AttentionDecoder 类定义了带有注意力机制的解码器基本接口。\n","class AttentionDecoder(d2l.Decoder):\n","    \"\"\"带有注意力机制的解码器基本接口\"\"\"\n","    def __init__(self, **kwargs):\n","        super(AttentionDecoder, self).__init__(**kwargs)\n","\n","    @property\n","    def attention_weights(self):\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3CFJoQqimdD"},"source":["#在每个解码时间步骤中，解码器上一个时间步的最终层隐藏状态将用作关注的查询。因此，注意力输出和输入嵌入都连接为循环神经网络解码器的输入。\n","class Seq2SeqAttentionDecoder(AttentionDecoder):\n","    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n","                 dropout=0, **kwargs):\n","        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n","        self.attention = d2l.AdditiveAttention(\n","            num_hiddens, num_hiddens, num_hiddens, dropout)\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.GRU(\n","            embed_size + num_hiddens, num_hiddens, num_layers,\n","            dropout=dropout)\n","        self.dense = nn.Linear(num_hiddens, vocab_size)\n","\n","    def init_state(self, enc_outputs, enc_valid_lens, *args):\n","        # `enc_outputs`的形状为 (`batch_size`, `num_steps`, `num_hiddens`).\n","        # `hidden_state[0]`的形状为 (`num_layers`, `batch_size`,\n","        # `num_hiddens`)\n","        outputs, hidden_state = enc_outputs\n","        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n","\n","    def forward(self, X, state):\n","        # `enc_outputs`的形状为 (`batch_size`, `num_steps`, `num_hiddens`).\n","        # `hidden_state[0]`的形状为 (`num_layers`, `batch_size`,\n","        # `num_hiddens`)\n","        enc_outputs, hidden_state, enc_valid_lens = state\n","        # 输出 `X`的形状为 (`num_steps`, `batch_size`, `embed_size`)\n","        X = self.embedding(X).permute(1, 0, 2)\n","        outputs, self._attention_weights = [], []\n","        for x in X:\n","            # `query`的形状为 (`batch_size`, 1, `num_hiddens`)\n","            query = torch.unsqueeze(hidden_state[-1], dim=1)\n","            # `context`的形状为 (`batch_size`, 1, `num_hiddens`)\n","            context = self.attention(\n","                query, enc_outputs, enc_outputs, enc_valid_lens)\n","            # 在特征维度上连结\n","            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n","            # 将 `x` 变形为 (1, `batch_size`, `embed_size` + `num_hiddens`)\n","            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n","            outputs.append(out)\n","            self._attention_weights.append(self.attention.attention_weights)\n","        # 全连接层变换后， `outputs`的形状为\n","        # (`num_steps`, `batch_size`, `vocab_size`)\n","        outputs = self.dense(torch.cat(outputs, dim=0))\n","        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n","                                          enc_valid_lens]\n","\n","    @property\n","    def attention_weights(self):\n","        return self._attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dgbr-i5Vipxa"},"source":["#接下来，我们使用包含 7 个时间步的 4 个序列输入的小批量测试Bahdanau 注意力解码器。\n","encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n","                             num_layers=2)\n","encoder.eval()\n","decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n","                                  num_layers=2)\n","decoder.eval()\n","X = torch.zeros((4, 7), dtype=torch.long)  # (`batch_size`, `num_steps`)\n","state = decoder.init_state(encoder(X), None)\n","output, state = decoder(X, state)\n","output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLaSp-2YirV6"},"source":["#10.4.3. 训练\n","#与 9.7.4节 类似，我们在这里指定超参数，实例化一个带有 Bahdanau 注意力的编码器和解码器，并对这个模型进行机器翻译训练。\n","#由于新增的注意力机制，这项训练要比没有注意力机制的 9.7.4节 慢得多。\n","embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n","batch_size, num_steps = 64, 10\n","lr, num_epochs, device = 0.005, 250, d2l.try_gpu()\n","\n","train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n","encoder = d2l.Seq2SeqEncoder(\n","    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n","decoder = Seq2SeqAttentionDecoder(\n","    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n","net = d2l.EncoderDecoder(encoder, decoder)\n","d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5frn60iEizpq"},"source":["#模型训练后，我们用它将几个英语句子翻译成法语并计算它们的 BLEU 分数。\n","engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n","fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n","for eng, fra in zip(engs, fras):\n","    translation, dec_attention_weight_seq = d2l.predict_seq2seq(\n","        net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n","    print(f'{eng} => {translation}, ',\n","          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57i49F-5i3yT"},"source":["attention_weights = torch.cat([step[0][0][0] for step in dec_attention_weight_seq], 0).reshape((\n","    1, 1, -1, num_steps))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxii68PQi4Li"},"source":["#训练结束后通过可视化注意力权重，我们可以看到，每个查询都会在键值对上分配不同的权重。\n","#它显示，在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。\n","# 加上一个包含序列结束词元\n","d2l.show_heatmaps(\n","    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),\n","    xlabel='Key posistions', ylabel='Query posistions')"],"execution_count":null,"outputs":[]}]}