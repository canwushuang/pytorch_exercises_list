{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.3. 注意力打分函数.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOAcDLw978kaLMcq/neRYAK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2GWKz-mshdDg"},"source":["#正如我们所看到的，选择不同的注意力打分函数  a  会导致不同的注意力汇聚操作。\n","#在本节中，我们将介绍两个流行的打分函数，稍后将用他们来实现更复杂的注意力机制。\n","!pip install d2l\n","import math\n","import torch\n","from torch import nn\n","from d2l import torch as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-AA3aIYhnjZ"},"source":["#10.3.1. 遮蔽softmax操作\n","#通过这种方式，我们可以在下面的 masked_softmax 函数中实现这样的 遮蔽 softmax 操作（masked softmax operation），\n","#其中任何超出有效长度的位置都被遮蔽并置为0。\n","def masked_softmax(X, valid_lens):\n","    \"\"\"通过在最后一个轴上遮蔽元素来执行 softmax 操作\"\"\"\n","    # `X`: 3D张量, `valid_lens`: 1D或2D 张量\n","    if valid_lens is None:\n","        return nn.functional.softmax(X, dim=-1)\n","    else:\n","        shape = X.shape\n","        if valid_lens.dim() == 1:\n","            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n","        else:\n","            valid_lens = valid_lens.reshape(-1)\n","        # 在最后的轴上，被遮蔽的元素使用一个非常大的负值替换，从而其 softmax (指数)输出为 0\n","        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n","                              value=-1e6)\n","        return nn.functional.softmax(X.reshape(shape), dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxTQwuiBhwAa"},"source":["#为了演示此函数是如何工作的，考虑由两个  2×4  矩阵表示的样本，\n","#这两个样本的有效长度分别为  2  和  3 。经过遮蔽 softmax 操作，超出有效长度的值都被遮蔽为0。\n","masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ye7490S4hybR"},"source":["#同样，我们也可以使用二维张量为矩阵样本中的每一行指定有效长度。\n","masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_WRiUkZh32q"},"source":["#10.3.2. 加性注意力\n","class AdditiveAttention(nn.Module):\n","    \"\"\"加性注意力\"\"\"\n","    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n","        super(AdditiveAttention, self).__init__(**kwargs)\n","        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n","        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n","        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, queries, keys, values, valid_lens):\n","        queries, keys = self.W_q(queries), self.W_k(keys)\n","        # 在维度扩展后，\n","        # `queries` 的形状：(`batch_size`, 查询的个数, 1, `num_hidden`)\n","        # `key` 的形状：(`batch_size`, 1, “键－值”对的个数, `num_hiddens`)\n","        # 使用广播方式进行求和\n","        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n","        features = torch.tanh(features)\n","        # `self.w_v` 仅有一个输出，因此从形状中移除最后那个维度。\n","        # `scores` 的形状：(`batch_size`, 查询的个数, “键-值”对的个数)\n","        scores = self.w_v(features).squeeze(-1)\n","        self.attention_weights = masked_softmax(scores, valid_lens)\n","        # `values` 的形状：(`batch_size`, “键－值”对的个数, 值的维度)\n","        return torch.bmm(self.dropout(self.attention_weights), values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JWa1dnAgh5uS"},"source":["#让我们用一个小例子来演示上面的AdditiveAttention类，其中查询、键和值的形状为（批量大小、步数或词元序列长度、特征大小），\n","#实际输出为  (2,1,20) 、 (2,10,2)  和  (2,10,4) 。注意力汇聚输出的形状为（批量大小、查询的步数、值的维度）。\n","queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n","# `values` 的小批量数据集中，两个值矩阵是相同的\n","values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n","    2, 1, 1)\n","valid_lens = torch.tensor([2, 6])\n","\n","attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n","                              dropout=0.1)\n","attention.eval()\n","attention(queries, keys, values, valid_lens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkuBa8JeiBmq"},"source":["#尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的，所以注意力权重是均匀的，由指定的有效长度决定。\n","d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel='Keys', ylabel='Queries')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spwIbDh6iFvy"},"source":["#10.3.3. 缩放点积注意力\n","class DotProductAttention(nn.Module):\n","    \"\"\"缩放点积注意力\"\"\"\n","    def __init__(self, dropout, **kwargs):\n","        super(DotProductAttention, self).__init__(**kwargs)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    # `queries` 的形状：(`batch_size`, 查询的个数, `d`)\n","    # `keys` 的形状：(`batch_size`, “键－值”对的个数, `d`)\n","    # `values` 的形状：(`batch_size`, “键－值”对的个数, 值的维度)\n","    # `valid_lens` 的形状: (`batch_size`,) 或者 (`batch_size`, 查询的个数)\n","    def forward(self, queries, keys, values, valid_lens=None):\n","        d = queries.shape[-1]\n","        # 设置 `transpose_b=True` 为了交换 `keys` 的最后两个维度\n","        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n","        self.attention_weights = masked_softmax(scores, valid_lens)\n","        return torch.bmm(self.dropout(self.attention_weights), values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Snd_9PdoiJLz"},"source":["#为了演示上述的DotProductAttention类，我们使用了与先前加性注意力例子中相同的键、值和有效长度。\n","#对于点积操作，令查询的特征维度与键的特征维度大小相同。\n","queries = torch.normal(0, 1, (2, 1, 2))\n","attention = DotProductAttention(dropout=0.5)\n","attention.eval()\n","attention(queries, keys, values, valid_lens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enjq0dYHiOKL"},"source":["#与加性注意力演示相同，由于键包含的是相同的元素，而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。\n","d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)), xlabel='Keys', ylabel='Queries')"],"execution_count":null,"outputs":[]}]}